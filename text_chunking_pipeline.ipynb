{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075ea9e2",
   "metadata": {},
   "source": [
    "# Amazon Textract\n",
    "\n",
    "Testing Amazon Textract but mostly having SubscriptionRequired errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31362e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from botocore.exceptions import ClientError, NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ba38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Textract Client Function\n",
    "def initialize_textract_client(region_name: str = 'us-east-1', debug: bool = False) -> boto3.client:\n",
    "    \"\"\"\n",
    "    Initialize AWS Textract client \n",
    "    \n",
    "    Args:\n",
    "        region_name (str): AWS region for Textract service\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        boto3.client: Configured Textract client\n",
    "    \n",
    "    Raises:\n",
    "        NoCredentialsError: If AWS credentials are not configured\n",
    "        Exception: For other AWS configuration issues\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Initializing Textract client for region: {region_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the Textract client\n",
    "        textract = boto3.client('textract', region_name=region_name)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"[DEBUG] Successfully created Textract client\")\n",
    "            # Test client by listing available operations (doesn't make API call)\n",
    "            operations = [op for op in dir(textract) if not op.startswith('_') and callable(getattr(textract, op))][:5]\n",
    "            print(f\"[DEBUG] Available operations sample: {operations}\")\n",
    "        \n",
    "        return textract\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        error_msg = \"AWS credentials not found. Please configure AWS credentials.\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to initialize Textract client: {str(e)}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25e61a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initializing Textract client for region: us-east-1\n",
      "[DEBUG] Successfully created Textract client\n",
      "[DEBUG] Available operations sample: ['analyze_document', 'analyze_expense', 'analyze_id', 'can_paginate', 'close']\n",
      "✅ Textract client initialized successfully!\n",
      "Service name: textract\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Test Textract Client Initialization\n",
    "try:\n",
    "    # Test the function with debug enabled\n",
    "    textract_client = initialize_textract_client(debug=True)\n",
    "    print(\"✅ Textract client initialized successfully!\")\n",
    "    print(f\"Service name: {textract_client._service_model.service_name}\")\n",
    "    print(f\"Region: {textract_client.meta.region_name}\")\n",
    "    \n",
    "except NoCredentialsError:\n",
    "    print(\"❌ AWS credentials not configured. Please set up your AWS credentials.\")\n",
    "    print(\"You can configure them using:\")\n",
    "    print(\"1. AWS CLI: aws configure\")\n",
    "    print(\"2. Environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "    print(\"3. IAM roles (if running on EC2)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Textract client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Directory Scanner Function\n",
    "def scan_pdf_directory(root_directory: str, debug: bool = False) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively scan directory for PDF files and return structured information.\n",
    "    \n",
    "    Args:\n",
    "        root_directory (str): Path to root directory containing PDFs\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of PDF file information with keys: 'path', 'name', 'size', 'subfolder'\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Scanning directory: {root_directory}\")\n",
    "    \n",
    "    pdf_files = []\n",
    "    root_path = Path(root_directory)\n",
    "    \n",
    "    if not root_path.exists():\n",
    "        error_msg = f\"Directory does not exist: {root_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        return pdf_files\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        error_msg = f\"Path is not a directory: {root_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        return pdf_files\n",
    "    \n",
    "    # Recursively find all PDF files\n",
    "    pdf_pattern = \"**/*.pdf\"\n",
    "    found_files = list(root_path.glob(pdf_pattern))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Found {len(found_files)} PDF files\")\n",
    "    \n",
    "    for pdf_path in found_files:\n",
    "        try:\n",
    "            # Get file stats\n",
    "            file_stats = pdf_path.stat()\n",
    "            \n",
    "            # Determine subfolder relative to root\n",
    "            relative_path = pdf_path.relative_to(root_path)\n",
    "            subfolder = str(relative_path.parent) if relative_path.parent != Path('.') else 'root'\n",
    "            \n",
    "            pdf_info = {\n",
    "                'path': str(pdf_path.absolute()),\n",
    "                'name': pdf_path.name,\n",
    "                'size': file_stats.st_size,\n",
    "                'subfolder': subfolder,\n",
    "                'size_mb': round(file_stats.st_size / (1024 * 1024), 2)\n",
    "            }\n",
    "            \n",
    "            pdf_files.append(pdf_info)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found PDF: {pdf_info['name']} ({pdf_info['size_mb']} MB) in {subfolder}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing file {pdf_path}: {str(e)}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    # Sort by subfolder then by name\n",
    "    pdf_files.sort(key=lambda x: (x['subfolder'], x['name']))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Successfully processed {len(pdf_files)} PDF files\")\n",
    "        \n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfaf17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for PDF files...\n",
      "[DEBUG] Scanning directory: ./pdfs\n",
      "[DEBUG] Found 1 PDF files\n",
      "[DEBUG] Found PDF: deepshield-systems-employee-handbook-2023.pdf (0.01 MB) in root\n",
      "[DEBUG] Successfully processed 1 PDF files\n",
      "\n",
      " Summary:\n",
      "Total PDFs found: 1\n",
      "Total size: 0.01 MB\n",
      "Subfolders: 1\n",
      "\n",
      "📁 Files by subfolder:\n",
      "  root: 1 files (0.01 MB)\n",
      "    - deepshield-systems-employee-handbook-2023.pdf (0.01 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Directory Scanner\n",
    "pdf_directory = \"./pdfs\"  # Change this to your actual directory path if needed\n",
    "\n",
    "print(\"Scanning for PDF files...\")\n",
    "pdf_files = scan_pdf_directory(pdf_directory, debug=True)\n",
    "\n",
    "print(f\"\\n Summary:\")\n",
    "print(f\"Total PDFs found: {len(pdf_files)}\")\n",
    "\n",
    "if pdf_files:\n",
    "    # Group by subfolder\n",
    "    subfolders = {}\n",
    "    total_size = 0\n",
    "    \n",
    "    for pdf in pdf_files:\n",
    "        subfolder = pdf['subfolder']\n",
    "        if subfolder not in subfolders:\n",
    "            subfolders[subfolder] = []\n",
    "        subfolders[subfolder].append(pdf)\n",
    "        total_size += pdf['size']\n",
    "    \n",
    "    print(f\"Total size: {round(total_size / (1024 * 1024), 2)} MB\")\n",
    "    print(f\"Subfolders: {len(subfolders)}\")\n",
    "    \n",
    "    print(\"\\n📁 Files by subfolder:\")\n",
    "    for subfolder, files in subfolders.items():\n",
    "        folder_size = sum(f['size'] for f in files)\n",
    "        print(f\"  {subfolder}: {len(files)} files ({round(folder_size / (1024 * 1024), 2)} MB)\")\n",
    "        \n",
    "        # Show first few files as examples\n",
    "        for i, file in enumerate(files[:3]):\n",
    "            print(f\"    - {file['name']} ({file['size_mb']} MB)\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "else:\n",
    "    print(\"❌ No PDF files found. Please check the directory path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a5f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Text Extraction Function using Textract\n",
    "def extract_text_from_pdf(pdf_path: str, textract_client: boto3.client, debug: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF using AWS Textract (synchronous processing).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        textract_client (boto3.client): Initialized Textract client\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Extraction results with keys: 'success', 'text', 'confidence', 'error', 'page_count'\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Processing PDF: {pdf_path}\")\n",
    "    \n",
    "    result = {\n",
    "        'success': False,\n",
    "        'text': '',\n",
    "        'confidence': 0.0,\n",
    "        'error': None,\n",
    "        'page_count': 0,\n",
    "        'file_size_mb': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        pdf_file = Path(pdf_path)\n",
    "        if not pdf_file.exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = pdf_file.stat().st_size\n",
    "        result['file_size_mb'] = round(file_size / (1024 * 1024), 2)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] File size: {result['file_size_mb']} MB\")\n",
    "        \n",
    "        # Check file size limit (10MB for synchronous processing)\n",
    "        if file_size > 10 * 1024 * 1024:\n",
    "            raise ValueError(f\"File too large for synchronous processing: {result['file_size_mb']} MB (max 10MB)\")\n",
    "        \n",
    "        # Read the PDF file\n",
    "        with open(pdf_path, 'rb') as document:\n",
    "            document_bytes = document.read()\n",
    "        \n",
    "        if debug:\n",
    "            print(\"[DEBUG] File read successfully, calling Textract...\")\n",
    "        \n",
    "        # Call Textract\n",
    "        response = textract_client.detect_document_text(\n",
    "            Document={'Bytes': document_bytes}\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Textract response received with {len(response.get('Blocks', []))} blocks\")\n",
    "        \n",
    "        # Extract text from blocks\n",
    "        extracted_text = []\n",
    "        confidences = []\n",
    "        \n",
    "        for block in response.get('Blocks', []):\n",
    "            if block['BlockType'] == 'LINE':\n",
    "                text = block.get('Text', '')\n",
    "                confidence = block.get('Confidence', 0)\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                confidences.append(confidence)\n",
    "                \n",
    "                if debug and len(extracted_text) <= 3:  # Show first few lines\n",
    "                    print(f\"[DEBUG] Line: '{text}' (confidence: {confidence:.1f}%)\")\n",
    "        \n",
    "        # Combine results\n",
    "        result['text'] = '\\n'.join(extracted_text)\n",
    "        result['confidence'] = sum(confidences) / len(confidences) if confidences else 0\n",
    "        result['page_count'] = len([block for block in response.get('Blocks', []) if block['BlockType'] == 'PAGE'])\n",
    "        result['success'] = True\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Extraction successful!\")\n",
    "            print(f\"[DEBUG] Pages: {result['page_count']}\")\n",
    "            print(f\"[DEBUG] Lines extracted: {len(extracted_text)}\")\n",
    "            print(f\"[DEBUG] Average confidence: {result['confidence']:.1f}%\")\n",
    "            print(f\"[DEBUG] Text length: {len(result['text'])} characters\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = f\"AWS Textract error ({error_code}): {e.response['Error']['Message']}\"\n",
    "        result['error'] = error_msg\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing PDF: {str(e)}\"\n",
    "        result['error'] = error_msg\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63f42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing PDF text extraction...\n",
      "\n",
      "📄 Test 1/1: deepshield-systems-employee-handbook-2023.pdf\n",
      "   Size: 0.01 MB | Subfolder: root\n",
      "[DEBUG] Processing PDF: c:\\Code\\Code for learning at Diligence\\Text Chunking and Processing\\PDF-Processing-Pipeline\\pdfs\\deepshield-systems-employee-handbook-2023.pdf\n",
      "[DEBUG] File size: 0.01 MB\n",
      "[DEBUG] File read successfully, calling Textract...\n",
      "[DEBUG] AWS Textract error (SubscriptionRequiredException): The AWS Access Key Id needs a subscription for the service\n",
      "   ❌ Failed: AWS Textract error (SubscriptionRequiredException): The AWS Access Key Id needs a subscription for the service\n",
      "------------------------------------------------------------\n",
      "\n",
      "📈 Test Summary:\n",
      "Files tested: 1\n",
      "Files suitable for sync processing: 1\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Text Extraction\n",
    "# First ensure we have a working Textract client\n",
    "try:\n",
    "    if 'textract_client' not in locals():\n",
    "        print(\"🔄 Initializing Textract client...\")\n",
    "        textract_client = initialize_textract_client(debug=True)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize Textract client: {e}\")\n",
    "    textract_client = None\n",
    "\n",
    "if 'pdf_files' in locals() and pdf_files and textract_client:\n",
    "    print(\"🧪 Testing PDF text extraction...\\n\")\n",
    "    \n",
    "    # Test on first few PDFs (or all if less than 3)\n",
    "    test_files = pdf_files[:min(3, len(pdf_files))]\n",
    "    \n",
    "    for i, pdf_info in enumerate(test_files, 1):\n",
    "        print(f\"📄 Test {i}/{len(test_files)}: {pdf_info['name']}\")\n",
    "        print(f\"   Size: {pdf_info['size_mb']} MB | Subfolder: {pdf_info['subfolder']}\")\n",
    "        \n",
    "        # Skip if file is too large for sync processing\n",
    "        if pdf_info['size_mb'] > 10:\n",
    "            print(\"   ⚠️  Skipping - File too large for synchronous processing (>10MB)\")\n",
    "            continue\n",
    "        \n",
    "        # Extract text\n",
    "        result = extract_text_from_pdf(pdf_info['path'], textract_client, debug=True)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"   ✅ Success!\")\n",
    "            print(f\"   📊 Pages: {result['page_count']} | Confidence: {result['confidence']:.1f}%\")\n",
    "            print(f\"   📝 Text length: {len(result['text'])} characters\")\n",
    "            \n",
    "            # Show first 200 characters of extracted text\n",
    "            preview_text = result['text'][:200].replace('\\n', ' ')\n",
    "            print(f\"   🔍 Preview: {preview_text}...\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ❌ Failed: {result['error']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Summary\n",
    "    successful_extractions = sum(1 for pdf in test_files if pdf['size_mb'] <= 10)\n",
    "    print(f\"\\n📈 Test Summary:\")\n",
    "    print(f\"Files tested: {len(test_files)}\")\n",
    "    print(f\"Files suitable for sync processing: {successful_extractions}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot run test - missing PDF files or Textract client\")\n",
    "    print(\"Please run the previous cells first to scan for PDFs and initialize the client.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d6d15",
   "metadata": {},
   "source": [
    "# PDF Extraction Uses PyMuPDF\n",
    "\n",
    "Since Textract isn't working, trying with PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9ee3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6607187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyMuPDF for PDF Extraction\n",
    "def extract_text_from_pdf(pdf_path, debug=False):\n",
    "    \"\"\"\n",
    "    Extract text from a single PDF file\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text from the PDF\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting text extraction for: {pdf_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF document\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] PDF opened successfully. Total pages: {doc.page_count}\")\n",
    "        \n",
    "        # Iterate through each page\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            # Extract text from the page\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            if debug:\n",
    "                char_count = len(page_text.strip())\n",
    "                print(f\"[DEBUG] Page {page_num + 1}: Extracted {char_count} characters\")\n",
    "            \n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "            text += page_text\n",
    "            text += \"\\n\"\n",
    "        \n",
    "        # Close the document\n",
    "        doc.close()\n",
    "        \n",
    "        if debug:\n",
    "            total_chars = len(text.strip())\n",
    "            print(f\"[DEBUG] Completed extraction. Total characters: {total_chars}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error extracting text from {pdf_path}: {str(e)}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return \"\"\n",
    "\n",
    "# Find PDF files in a directory (Don't care about file size unlike other function)\n",
    "def find_pdf_files(directory, debug=False):\n",
    "    \"\"\"\n",
    "    Find all PDF files in directory and subdirectories\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Root directory to search\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        list: List of PDF file paths\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Searching for PDF files in: {directory}\")\n",
    "    \n",
    "    pdf_files = []\n",
    "    directory_path = Path(directory)\n",
    "    \n",
    "    if not directory_path.exists():\n",
    "        error_msg = f\"Directory {directory} does not exist\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return []\n",
    "    \n",
    "    # Recursively find all PDF files\n",
    "    for pdf_file in directory_path.rglob(\"*.pdf\"):\n",
    "        if pdf_file.is_file():\n",
    "            pdf_files.append(str(pdf_file))\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found PDF: {pdf_file}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total PDF files found: {len(pdf_files)}\")\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "def extract_pdfs_from_directory(input_directory, output_file=None, save_individual=False, debug=False):\n",
    "    \"\"\"\n",
    "    Extract text from all PDFs in a directory and its subdirectories\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Directory containing PDF files\n",
    "        output_file (str, optional): Path to save combined extracted text\n",
    "        save_individual (bool): Whether to save each PDF's text individually\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with PDF paths as keys and extracted text as values\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting PDF extraction from directory: {input_directory}\")\n",
    "        print(f\"[DEBUG] Output file: {output_file}\")\n",
    "        print(f\"[DEBUG] Save individual files: {save_individual}\")\n",
    "    \n",
    "    if not os.path.exists(input_directory):\n",
    "        error_msg = f\"Directory {input_directory} does not exist\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return {}\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = find_pdf_files(input_directory, debug=debug)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        warning_msg = f\"No PDF files found in {input_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG WARNING] {warning_msg}\")\n",
    "        else:\n",
    "            print(warning_msg)\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    extracted_texts = {}\n",
    "    all_text = \"\"\n",
    "    successful_extractions = 0\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Processing file {i}/{len(pdf_files)}: {pdf_path}\")\n",
    "        else:\n",
    "            print(f\"Processing ({i}/{len(pdf_files)}): {os.path.basename(pdf_path)}\")\n",
    "        \n",
    "        # Extract text from PDF\n",
    "        text = extract_text_from_pdf(pdf_path, debug=debug)\n",
    "        \n",
    "        if text.strip():  # Check if there's actual content\n",
    "            extracted_texts[pdf_path] = text\n",
    "            successful_extractions += 1\n",
    "            \n",
    "            # Add to combined text\n",
    "            all_text += f\"\\n{'='*50}\\n\"\n",
    "            all_text += f\"FILE: {pdf_path}\\n\"\n",
    "            all_text += f\"{'='*50}\\n\"\n",
    "            all_text += text\n",
    "            all_text += \"\\n\"\n",
    "            \n",
    "            # Save individual file if requested\n",
    "            if save_individual:\n",
    "                output_path = pdf_path.replace('.pdf', '_extracted.txt')\n",
    "                try:\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(text)\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Saved individual text to: {output_path}\")\n",
    "                    else:\n",
    "                        print(f\"  → Saved individual text file\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error saving individual file {output_path}: {str(e)}\"\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "                    else:\n",
    "                        print(f\"  → Error saving individual file: {str(e)}\")\n",
    "        \n",
    "        else:\n",
    "            warning_msg = f\"No text extracted from: {pdf_path}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG WARNING] {warning_msg}\")\n",
    "            else:\n",
    "                print(f\"  → No text extracted\")\n",
    "    \n",
    "    # Save combined text if output file specified\n",
    "    if output_file and all_text:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(all_text)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Saved combined text to: {output_file}\")\n",
    "            else:\n",
    "                print(f\"Saved combined text to: {output_file}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error saving combined file {output_file}: {str(e)}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "            else:\n",
    "                print(f\"Error saving combined file: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nExtraction complete!\")\n",
    "    print(f\"Successfully processed {successful_extractions}/{len(pdf_files)} PDF files\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total characters in combined text: {len(all_text)}\")\n",
    "        print(f\"[DEBUG] Extraction results dictionary has {len(extracted_texts)} entries\")\n",
    "    \n",
    "    return extracted_texts\n",
    "\n",
    "def simple_extract(directory_path, debug=False):\n",
    "    \"\"\"\n",
    "    Simple function to extract text from all PDFs in a directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing PDFs\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with PDF paths as keys and extracted text as values\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Simple extract called for directory: {directory_path}\")\n",
    "    \n",
    "    return extract_pdfs_from_directory(directory_path, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3330b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files\n",
      "Processing (1/1): deepshield-systems-employee-handbook-2023.pdf\n",
      "\n",
      "Extraction complete!\n",
      "Successfully processed 1/1 PDF files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdfs\\\\deepshield-systems-employee-handbook-2023.pdf': '\\n--- Page 1 ---\\nDeepShield Systems Employee Handbook 2023\\n1. Introduction and Purpose\\n1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\\nconditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the\\n\"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400,\\nWilmington, DE 19801.\\n2. This Handbook supersedes all previous employee handbooks and management memos. The\\npolicies contained herein are effective as of January 1, 2023.\\n2. Employment Policies\\n1. Equal Employment Opportunity\\nDeepShield provides equal employment opportunities to all employees and applicants without regard\\nto race, color, religion, sex, national origin, age, disability, genetic information, or any other\\nprotected characteristic.\\n2. Employment Classification\\n-\\nFull-time Regular: Employees scheduled to work 40 hours per week\\n-\\nPart-time Regular: Employees scheduled to work less than 40 hours per week\\n-\\nTemporary: Employees hired for a specific project or time period\\n-\\nExempt: Salaried employees exempt from overtime requirements\\n-\\nNon-exempt: Employees eligible for overtime compensation\\n3. Security Clearance Requirements\\nGiven the nature of DeepShield\\'s work in critical infrastructure protection, certain positions may\\nrequire government security clearances. Employees must maintain required clearances as a condition\\nof employment.\\n3. Workplace Policies\\n1. Confidentiality and Intellectual Property\\n-\\nAll employees must sign and comply with the Company\\'s Confidentiality and Intellectual\\nProperty Agreement\\n\\n\\n--- Page 2 ---\\n-\\nProprietary information includes but is not limited to: source code, security architectures,\\ncustomer data, threat detection algorithms, and maritime protection protocols\\n-\\nEmployees must report any potential security breaches or unauthorized access immediately\\n2. Cybersecurity Compliance\\n-\\nMandatory participation in quarterly cybersecurity training\\n-\\nStrict adherence to the Company\\'s \"Clean Screen\" policy\\n-\\nTwo-factor authentication required for all system access\\n-\\nProhibition on use of unauthorized software or devices\\n3. Remote Work Policy\\n-\\nEligible employees may work remotely subject to manager approval\\n-\\nRemote work must comply with DeepShield\\'s security protocols\\n-\\nVPN and encrypted connections required for all remote access\\n-\\nRegular office presence required for collaborative projects\\n4. Compensation and Benefits\\n1. Compensation\\n-\\nPay periods: Semi-monthly on the 15th and last day of each month\\n-\\nPerformance reviews conducted annually\\n-\\nMerit increases based on performance and market conditions\\n-\\nEquity compensation available per individual agreements\\n2. Benefits Package\\n-\\nMedical, dental, and vision insurance\\n-\\n401(k) plan with company match up to 4%\\n-\\nLife and disability insurance\\n-\\nProfessional development allowance ($5,000 annually)\\n-\\nSecurity certification reimbursement program\\n5. Time Off and Leave Policies\\n1. Paid Time Off (PTO)\\n-\\n0-2 years: 15 days annually\\n\\n\\n--- Page 3 ---\\n-\\n3-5 years: 20 days annually\\n-\\n6+ years: 25 days annually\\n-\\nMaximum accrual cap: 1.5x annual allowance\\n2. Other Leave\\n-\\nBereavement leave: Up to 5 days\\n-\\nJury duty: Paid as required by law\\n-\\nMilitary leave: In accordance with USERRA\\n-\\nParental leave: 12 weeks paid at 100% salary\\n6. Code of Conduct\\n1. Professional Standards\\n-\\nMaintain highest ethical standards\\n-\\nReport conflicts of interest\\n-\\nComply with all applicable laws and regulations\\n-\\nProtect company and customer confidential information\\n2. Workplace Safety\\n-\\nReport safety concerns immediately\\n-\\nFollow all security protocols\\n-\\nZero tolerance for workplace violence\\n-\\nMaintain required certifications\\n7. Acknowledgment and Disclaimer\\n1. This Handbook is not a contract of employment and does not create contractual obligations of any\\nkind between DeepShield and any employee.\\n2. DeepShield reserves the right to modify, revoke, suspend, terminate, or change any guidelines,\\npolicies, or procedures, in whole or in part, at any time, with or without notice.\\n3. Employment at DeepShield is \"at-will,\" meaning either the employee or DeepShield may\\nterminate the employment relationship at any time, with or without cause or notice.\\n8. Execution\\n\\n\\n--- Page 4 ---\\nI acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply\\nwith its contents.\\nEmployee Name: _\\nEmployee Signature: \\nDate: _\\nAuthorized by:\\n/s/ Dr. Marcus Chen\\nChief Executive Officer\\nDeepShield Systems, Inc.\\nDate: January 1, 2023\\n\\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_texts = simple_extract(r\".\\pdfs\")\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c02f86",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961d71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27740b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning function\n",
    "def clean_text_for_rag(text: str, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess text for LangChain RAG system.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to be cleaned\n",
    "        debug (bool): If True, prints debug information for each cleaning step\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text ready for RAG processing\n",
    "    \"\"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting text cleaning. Original length: {len(text)} characters\")\n",
    "        print(f\"[DEBUG] Original text preview: {text[:200]}...\")\n",
    "    \n",
    "    # Step 1: Remove page headers and footers\n",
    "    original_text = text\n",
    "    text = re.sub(r'--- Page \\d+ ---\\n?', '', text)\n",
    "    if debug:\n",
    "        removed_pages = len(re.findall(r'--- Page \\d+ ---', original_text))\n",
    "        print(f\"[DEBUG] Step 1: Removed {removed_pages} page markers\")\n",
    "    \n",
    "    # Step 2: Clean up signature lines and formatting artifacts first\n",
    "    before_artifacts = text\n",
    "    text = re.sub(r'^[-_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'_{3,}', '[SIGNATURE LINE]', text)\n",
    "    if debug:\n",
    "        artifacts_removed = len(re.findall(r'^[-_]{3,}\\s*$', before_artifacts, flags=re.MULTILINE))\n",
    "        print(f\"[DEBUG] Step 2: Removed {artifacts_removed} formatting artifact lines\")\n",
    "    \n",
    "    # Step 3: Normalize basic whitespace\n",
    "    # Replace multiple spaces with single space\n",
    "    before_spaces = text\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    if debug:\n",
    "        spaces_removed = len(before_spaces) - len(text)\n",
    "        print(f\"[DEBUG] Step 3: Removed {spaces_removed} extra spaces\")\n",
    "    \n",
    "    # Step 4: Identify and preserve section structure\n",
    "    # First, ensure section headers and subsection headers are on their own lines\n",
    "    # Handle main sections (1. Title, 2. Title, etc.)\n",
    "    text = re.sub(r'(?<!\\n)(\\d+\\.\\s+[A-Z][^\\n]*)', r'\\n\\n\\1', text)\n",
    "    \n",
    "    # Handle subsections (1.1, 1.2, 2.1, etc.)\n",
    "    text = re.sub(r'(?<!\\n)(\\d+\\.\\d+\\.?\\s+[A-Z][^\\n]*)', r'\\n\\n\\1', text)\n",
    "    \n",
    "    # Handle sub-subsections (1.1.1, 1.2.3, etc.)\n",
    "    text = re.sub(r'(?<!\\n)(\\d+\\.\\d+\\.\\d+\\.?\\s+[A-Z][^\\n]*)', r'\\n\\n\\1', text)\n",
    "    \n",
    "    if debug:\n",
    "        sections_found = len(re.findall(r'^\\d+\\.\\s+[A-Z]', text, flags=re.MULTILINE))\n",
    "        subsections_found = len(re.findall(r'^\\d+\\.\\d+\\.?\\s+[A-Z]', text, flags=re.MULTILINE))\n",
    "        subsubsections_found = len(re.findall(r'^\\d+\\.\\d+\\.\\d+\\.?\\s+[A-Z]', text, flags=re.MULTILINE))\n",
    "        print(f\"[DEBUG] Step 4: Found {sections_found} sections, {subsections_found} subsections, {subsubsections_found} sub-subsections\")\n",
    "    \n",
    "    # Step 5: Handle paragraph breaks and line joining more intelligently\n",
    "    # First, normalize multiple newlines to double newlines (paragraph breaks)\n",
    "    before_newlines = text\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "    if debug:\n",
    "        newlines_normalized = len(re.findall(r'\\n\\s*\\n\\s*\\n+', before_newlines))\n",
    "        print(f\"[DEBUG] Step 5a: Normalized {newlines_normalized} multiple newline sequences\")\n",
    "    \n",
    "    # Join lines within paragraphs, but preserve structure\n",
    "    # This is critical for proper subsection numbering later\n",
    "    lines = text.split('\\n')\n",
    "    joined_lines = []\n",
    "    i = 0\n",
    "    line_joins = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        current_line = lines[i].strip()\n",
    "        \n",
    "        if not current_line:\n",
    "            # Empty line - preserve it as paragraph separator\n",
    "            joined_lines.append('')\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Check if this is a section header that should stay separate\n",
    "        is_main_section = re.match(r'^\\d+\\.\\s+[A-Z]', current_line)\n",
    "        is_existing_subsection = re.match(r'^\\d+\\.\\d+\\.?\\s+', current_line)\n",
    "        is_list_item = re.match(r'^\\s*[-•*]\\s+', current_line)\n",
    "        is_signature_line = '[SIGNATURE LINE]' in current_line\n",
    "        \n",
    "        if is_main_section or is_existing_subsection or is_list_item or is_signature_line:\n",
    "            # These should stay on their own lines\n",
    "            joined_lines.append(current_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # This could be the start of a paragraph - look ahead to join continuation lines\n",
    "            paragraph_lines = [current_line]\n",
    "            i += 1\n",
    "            \n",
    "            # Look ahead and join lines that appear to be continuations\n",
    "            while i < len(lines):\n",
    "                next_line = lines[i].strip()\n",
    "                \n",
    "                if not next_line:\n",
    "                    # Empty line ends the paragraph\n",
    "                    break\n",
    "                    \n",
    "                # Check if next line should stay separate\n",
    "                next_is_main_section = re.match(r'^\\d+\\.\\s+[A-Z]', next_line)\n",
    "                next_is_subsection = re.match(r'^\\d+\\.\\d+\\.?\\s+', next_line)\n",
    "                next_is_list = re.match(r'^\\s*[-•*]\\s+', next_line)\n",
    "                next_is_signature = '[SIGNATURE LINE]' in next_line\n",
    "                \n",
    "                # Stop joining if we hit a structural element\n",
    "                if next_is_main_section or next_is_subsection or next_is_list or next_is_signature:\n",
    "                    break\n",
    "                \n",
    "                # Check if this looks like a continuation of the current paragraph\n",
    "                # Continue if the line doesn't start with a capital letter (likely continuation)\n",
    "                # or if it's clearly part of the same sentence\n",
    "                looks_like_continuation = (\n",
    "                    not re.match(r'^[A-Z][a-z]', next_line) or  # Doesn't start with capital\n",
    "                    len(next_line) > 100 or  # Long line likely to be content\n",
    "                    paragraph_lines[-1].endswith(',') or  # Previous line ends with comma\n",
    "                    paragraph_lines[-1].endswith('and') or  # Previous line ends with 'and'\n",
    "                    paragraph_lines[-1].endswith('or')  # Previous line ends with 'or'\n",
    "                )\n",
    "                \n",
    "                if looks_like_continuation:\n",
    "                    # Join this line to the paragraph\n",
    "                    paragraph_lines.append(next_line)\n",
    "                    line_joins += 1\n",
    "                    i += 1\n",
    "                else:\n",
    "                    # This looks like it might be the start of a new paragraph\n",
    "                    break\n",
    "            \n",
    "            # Join the paragraph lines with spaces\n",
    "            joined_lines.append(' '.join(paragraph_lines))\n",
    "    \n",
    "    text = '\\n'.join(joined_lines)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 5b: Joined {line_joins} lines into paragraphs while preserving structure\")\n",
    "    \n",
    "    # Step 4.5: Auto-number content paragraphs as subsections within numbered sections\n",
    "    # This handles cases where numbered sections are followed by unnumbered paragraphs\n",
    "    # that should become subsections (1.1, 1.2, etc.)\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    current_section = None\n",
    "    subsection_counter = 1\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Check if this is a main section header\n",
    "        main_section_match = re.match(r'^(\\d+)\\.\\s+(.+)', line_stripped)\n",
    "        if main_section_match:\n",
    "            current_section = main_section_match.group(1)\n",
    "            subsection_counter = 1\n",
    "            processed_lines.append(line)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found main section: {current_section}. {main_section_match.group(2)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if this is already a properly numbered subsection\n",
    "        existing_subsection = re.match(r'^\\d+\\.\\d+\\.?\\s+', line_stripped)\n",
    "        if existing_subsection:\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "            \n",
    "        # Check if this is an empty line\n",
    "        if not line_stripped:\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "            \n",
    "        # Check if this is a bullet point or list item (should stay as content)\n",
    "        if re.match(r'^[-•*]\\s+', line_stripped):\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # If we're under a main section and this is a content paragraph\n",
    "        # (not already numbered, not a bullet, not empty, and substantive content)\n",
    "        if (current_section and \n",
    "            line_stripped and \n",
    "            not re.match(r'^\\d+\\.', line_stripped) and  # Not already numbered\n",
    "            len(line_stripped) > 20):  # Has substantial content (avoid numbering short fragments)\n",
    "            \n",
    "            # This should become a subsection\n",
    "            numbered_line = f\"{current_section}.{subsection_counter} {line_stripped}\"\n",
    "            processed_lines.append(numbered_line)\n",
    "            subsection_counter += 1\n",
    "            if debug:\n",
    "                preview = line_stripped[:60] + \"...\" if len(line_stripped) > 60 else line_stripped\n",
    "                print(f\"[DEBUG] Auto-numbered paragraph as subsection {current_section}.{subsection_counter-1}: '{preview}'\")\n",
    "        else:\n",
    "            # This is other content - keep as is\n",
    "            processed_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(processed_lines)\n",
    "    \n",
    "    # Step 6: Standardize list formatting\n",
    "    before_bullets = text\n",
    "    text = re.sub(r'^\\s*[-•*]\\s*', '• ', text, flags=re.MULTILINE)\n",
    "    if debug:\n",
    "        bullets_standardized = len(re.findall(r'^\\s*[-•*]\\s*', before_bullets, flags=re.MULTILINE))\n",
    "        print(f\"[DEBUG] Step 6: Standardized {bullets_standardized} bullet points\")\n",
    "    \n",
    "    # Step 7: Clean up section numbering spacing\n",
    "    before_numbering = text\n",
    "    text = re.sub(r'^(\\d+\\.(\\d+\\.)*)\\s*([A-Z])', r'\\1 \\3', text, flags=re.MULTILINE)\n",
    "    if debug:\n",
    "        numbering_fixed = len(re.findall(r'^(\\d+\\.(\\d+\\.)*)\\s*([A-Z])', before_numbering, flags=re.MULTILINE))\n",
    "        print(f\"[DEBUG] Step 7: Fixed spacing for {numbering_fixed} section numbers\")\n",
    "    \n",
    "    # Step 8: Clean up punctuation\n",
    "    before_periods = text\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    periods_fixed = len(re.findall(r'\\.{2,}', before_periods))\n",
    "    \n",
    "    # Normalize spacing around punctuation\n",
    "    text = re.sub(r'\\s*:\\s*', ': ', text)\n",
    "    text = re.sub(r'\\s*;\\s*', '; ', text)\n",
    "    text = re.sub(r'\\s*,\\s*', ', ', text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 8: Fixed {periods_fixed} multiple periods and normalized punctuation spacing\")\n",
    "    \n",
    "    # Step 9: Final cleanup\n",
    "    # Remove excessive empty lines while preserving paragraph structure\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    prev_empty = False\n",
    "    empty_lines_removed = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "            prev_empty = False\n",
    "        elif not prev_empty:\n",
    "            cleaned_lines.append('')\n",
    "            prev_empty = True\n",
    "        else:\n",
    "            empty_lines_removed += 1\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # Remove any remaining multiple spaces and trim\n",
    "    before_final_spaces = text\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    final_spaces_removed = len(before_final_spaces) - len(text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 9: Final cleanup - removed {empty_lines_removed} excess empty lines, {final_spaces_removed} extra spaces\")\n",
    "        print(f\"[DEBUG] Cleaning complete. Final length: {len(text)} characters\")\n",
    "        print(f\"[DEBUG] Size reduction: {len(original_text) - len(text)} characters ({((len(original_text) - len(text)) / len(original_text) * 100):.1f}%)\")\n",
    "        print(f\"[DEBUG] Final text preview: {text[:200]}...\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeefbf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepShield Systems Employee Handbook 2023\\n1. Introduction and Purpose\\n1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\\n1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.\\n2. This Handbook supersedes all previous employee handbooks and management memos. The\\n2.1 policies contained herein are effective as of January 1, 2023.\\n2. Employment Policies\\n1. Equal Employment Opportunity\\n1.1 DeepShield provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, genetic information, or any other protected characteristic.\\n2. Employment Classification\\n• 2.1 Full-time Regular: Employees scheduled to work 40 hours per week -\\n2.2 Part-time Regular: Employees scheduled to work less than 40 hours per week -\\n2.3 Temporary: Employees hired for a specific project or time period -\\n2.4 Exempt: Salaried employees exempt from overtime requirements -\\n2.5 Non-exempt: Employees eligible for overtime compensation\\n3. Security Clearance Requirements\\n3.1 Given the nature of DeepShield\\'s work in critical infrastructure protection, certain positions may require government security clearances. Employees must maintain required clearances as a condition of employment.\\n3. Workplace Policies\\n1. Confidentiality and Intellectual Property\\n• 1.1 All employees must sign and comply with the Company\\'s Confidentiality and Intellectual\\nProperty Agreement\\n• 1.2 Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols -\\n1.3 Employees must report any potential security breaches or unauthorized access immediately\\n2. Cybersecurity Compliance\\n• 2.1 Mandatory participation in quarterly cybersecurity training -\\n2.2 Strict adherence to the Company\\'s \"Clean Screen\" policy -\\n2.3 Two-factor authentication required for all system access -\\n2.4 Prohibition on use of unauthorized software or devices\\n3. Remote Work Policy\\n• 3.1 Eligible employees may work remotely subject to manager approval -\\n3.2 Remote work must comply with DeepShield\\'s security protocols - VPN and encrypted connections required for all remote access -\\n3.3 Regular office presence required for collaborative projects\\n4. Compensation and Benefits\\n1. Compensation\\n• 1.1 Pay periods: Semi-monthly on the 15th and last day of each month -\\n1.2 Performance reviews conducted annually -\\n1.3 Merit increases based on performance and market conditions -\\n1.4 Equity compensation available per individual agreements\\n2. Benefits Package\\n• 2.1 Medical, dental, and vision insurance - 401(k) plan with company match up to 4% -\\n2.2 Life and disability insurance -\\n2.3 Professional development allowance ($5, 000 annually) -\\n2.4 Security certification reimbursement program\\n5. Time Off and Leave Policies\\n1. Paid Time Off (PTO)\\n• 0-2 years: 15 days annually\\n• 3-5 years: 20 days annually - 6+ years: 25 days annually -\\n1.1 Maximum accrual cap: 1.5x annual allowance\\n2. Other Leave\\n• 2.1 Bereavement leave: Up to 5 days -\\n2.2 Jury duty: Paid as required by law -\\n2.3 Military leave: In accordance with USERRA -\\n2.4 Parental leave: 12 weeks paid at 100% salary\\n6. Code of Conduct\\n1. Professional Standards\\n• 1.1 Maintain highest ethical standards -\\n1.2 Report conflicts of interest -\\n1.3 Comply with all applicable laws and regulations -\\n1.4 Protect company and customer confidential information\\n2. Workplace Safety\\n• 2.1 Report safety concerns immediately -\\n2.2 Follow all security protocols -\\n2.3 Zero tolerance for workplace violence -\\n2.4 Maintain required certifications\\n7. Acknowledgment and Disclaimer\\n1. This Handbook is not a contract of employment and does not create contractual obligations of any\\n1.1 kind between DeepShield and any employee.\\n2. DeepShield reserves the right to modify, revoke, suspend, terminate, or change any guidelines, 2.1 policies, or procedures, in whole or in part, at any time, with or without notice.\\n3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may\\n3.1 terminate the employment relationship at any time, with or without cause or notice.\\n8. Execution\\n\\n8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.\\nEmployee Name: _\\nEmployee Signature: Date: _\\n8.2 Authorized by: /s/ Dr. Marcus Chen\\n8.3 Chief Executive Officer\\n8.4 DeepShield Systems, Inc.\\n8.5 Date: January 1, 2023'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = clean_text_for_rag(next(iter(extracted_texts.values())), debug=False)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb50876",
   "metadata": {},
   "source": [
    "# Text Chunking not using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac0861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk by section headers\n",
    "def chunk_document_by_sections(cleaned_text: str, debug: bool = False) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split the cleaned document into logical chunks based on sections.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_text (str): Cleaned text from clean_text_for_rag()\n",
    "        debug (bool): If True, prints debug information about chunking process\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Starting document chunking\")\n",
    "        print(f\"[DEBUG] Input text length: {len(cleaned_text)} characters\")\n",
    "        print(f\"[DEBUG] Input text lines: {len(cleaned_text.splitlines())}\")\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # First, let's see what section headers we can find\n",
    "    section_headers = re.findall(r'^(\\d+\\.\\s+[^\\n]+)', cleaned_text, flags=re.MULTILINE)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Found potential section headers:\")\n",
    "        for i, header in enumerate(section_headers):\n",
    "            print(f\"[DEBUG]   {i+1}: '{header}'\")\n",
    "    \n",
    "    # Split by major sections (numbered sections like \"1. Introduction\")\n",
    "    # Use a more flexible pattern that captures the full header line\n",
    "    sections = re.split(r'^(\\d+\\.\\s+[^\\n]*)', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Split resulted in {len(sections)} segments\")\n",
    "        for i, segment in enumerate(sections[:6]):  # Show first 6 segments\n",
    "            segment_preview = segment.strip()[:100].replace('\\n', '\\\\n')\n",
    "            print(f\"[DEBUG]   Segment {i}: '{segment_preview}...'\")\n",
    "    \n",
    "    current_section = \"Document Header\"\n",
    "    current_content = \"\"\n",
    "    segment_count = 0\n",
    "    \n",
    "    for i, segment in enumerate(sections):\n",
    "        segment = segment.strip()\n",
    "        \n",
    "        if not segment:  # Skip empty segments\n",
    "            continue\n",
    "            \n",
    "        # Check if this segment is a section header\n",
    "        if re.match(r'^\\d+\\.\\s+[A-Za-z]', segment):\n",
    "            # This is a section header - save previous section if it has content\n",
    "            if current_content.strip():\n",
    "                chunk_length = len(current_content.strip())\n",
    "                chunks.append({\n",
    "                    'section': current_section,\n",
    "                    'content': current_content.strip(),\n",
    "                    'chunk_id': len(chunks)\n",
    "                })\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Created chunk {len(chunks)-1}: '{current_section}' ({chunk_length} chars)\")\n",
    "            \n",
    "            # Start new section\n",
    "            current_section = segment\n",
    "            current_content = \"\"\n",
    "            segment_count += 1\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Starting new section: '{current_section}'\")\n",
    "        else:\n",
    "            # This is section content - add to current section\n",
    "            if current_content:\n",
    "                current_content += \"\\n\\n\" + segment\n",
    "            else:\n",
    "                current_content = segment\n",
    "            if debug and len(segment) > 50:\n",
    "                print(f\"[DEBUG] Added content to '{current_section[:30]}...': {len(segment)} chars\")\n",
    "    \n",
    "    # Add the last section if it has content\n",
    "    if current_content.strip():\n",
    "        chunk_length = len(current_content.strip())\n",
    "        chunks.append({\n",
    "            'section': current_section,\n",
    "            'content': current_content.strip(),\n",
    "            'chunk_id': len(chunks)\n",
    "        })\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Created final chunk {len(chunks)-1}: '{current_section}' ({chunk_length} chars)\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Chunking summary:\")\n",
    "        print(f\"[DEBUG] - Found {len(section_headers)} section headers in text\")\n",
    "        print(f\"[DEBUG] - Processed {segment_count} sections\")\n",
    "        print(f\"[DEBUG] - Created {len(chunks)} chunks\")\n",
    "        \n",
    "        total_content_length = sum(len(chunk['content']) for chunk in chunks)\n",
    "        avg_chunk_length = total_content_length / len(chunks) if chunks else 0\n",
    "        print(f\"[DEBUG] - Total content length: {total_content_length} characters\")\n",
    "        print(f\"[DEBUG] - Average chunk length: {avg_chunk_length:.1f} characters\")\n",
    "        \n",
    "        # Show chunk size distribution\n",
    "        chunk_sizes = [len(chunk['content']) for chunk in chunks]\n",
    "        if chunk_sizes:\n",
    "            print(f\"[DEBUG] - Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "            \n",
    "        # Show all chunk sections\n",
    "        print(f\"[DEBUG] Final chunks created:\")\n",
    "        for chunk in chunks:\n",
    "            print(f\"[DEBUG]   Chunk {chunk['chunk_id']}: '{chunk['section'][:60]}...' ({len(chunk['content'])} chars)\")\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6939488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'section': 'Document Header',\n",
       "  'content': 'DeepShield Systems Employee Handbook 2023',\n",
       "  'chunk_id': 0},\n",
       " {'section': '1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working',\n",
       "  'content': '1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.',\n",
       "  'chunk_id': 1},\n",
       " {'section': '2. This Handbook supersedes all previous employee handbooks and management memos. The',\n",
       "  'content': '2.1 policies contained herein are effective as of January 1, 2023.',\n",
       "  'chunk_id': 2},\n",
       " {'section': '1. Equal Employment Opportunity',\n",
       "  'content': '1.1 DeepShield provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, genetic information, or any other protected characteristic.',\n",
       "  'chunk_id': 3},\n",
       " {'section': '2. Employment Classification',\n",
       "  'content': '• 2.1 Full-time Regular: Employees scheduled to work 40 hours per week -\\n2.2 Part-time Regular: Employees scheduled to work less than 40 hours per week -\\n2.3 Temporary: Employees hired for a specific project or time period -\\n2.4 Exempt: Salaried employees exempt from overtime requirements -\\n2.5 Non-exempt: Employees eligible for overtime compensation',\n",
       "  'chunk_id': 4},\n",
       " {'section': '3. Security Clearance Requirements',\n",
       "  'content': \"3.1 Given the nature of DeepShield's work in critical infrastructure protection, certain positions may require government security clearances. Employees must maintain required clearances as a condition of employment.\",\n",
       "  'chunk_id': 5},\n",
       " {'section': '1. Confidentiality and Intellectual Property',\n",
       "  'content': \"• 1.1 All employees must sign and comply with the Company's Confidentiality and Intellectual\\nProperty Agreement\\n• 1.2 Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols -\\n1.3 Employees must report any potential security breaches or unauthorized access immediately\",\n",
       "  'chunk_id': 6},\n",
       " {'section': '2. Cybersecurity Compliance',\n",
       "  'content': '• 2.1 Mandatory participation in quarterly cybersecurity training -\\n2.2 Strict adherence to the Company\\'s \"Clean Screen\" policy -\\n2.3 Two-factor authentication required for all system access -\\n2.4 Prohibition on use of unauthorized software or devices',\n",
       "  'chunk_id': 7},\n",
       " {'section': '3. Remote Work Policy',\n",
       "  'content': \"• 3.1 Eligible employees may work remotely subject to manager approval -\\n3.2 Remote work must comply with DeepShield's security protocols - VPN and encrypted connections required for all remote access -\\n3.3 Regular office presence required for collaborative projects\",\n",
       "  'chunk_id': 8},\n",
       " {'section': '1. Compensation',\n",
       "  'content': '• 1.1 Pay periods: Semi-monthly on the 15th and last day of each month -\\n1.2 Performance reviews conducted annually -\\n1.3 Merit increases based on performance and market conditions -\\n1.4 Equity compensation available per individual agreements',\n",
       "  'chunk_id': 9},\n",
       " {'section': '2. Benefits Package',\n",
       "  'content': '• 2.1 Medical, dental, and vision insurance - 401(k) plan with company match up to 4% -\\n2.2 Life and disability insurance -\\n2.3 Professional development allowance ($5, 000 annually) -\\n2.4 Security certification reimbursement program',\n",
       "  'chunk_id': 10},\n",
       " {'section': '1. Paid Time Off (PTO)',\n",
       "  'content': '• 0-2 years: 15 days annually\\n• 3-5 years: 20 days annually - 6+ years: 25 days annually -\\n1.1 Maximum accrual cap: 1.5x annual allowance',\n",
       "  'chunk_id': 11},\n",
       " {'section': '2. Other Leave',\n",
       "  'content': '• 2.1 Bereavement leave: Up to 5 days -\\n2.2 Jury duty: Paid as required by law -\\n2.3 Military leave: In accordance with USERRA -\\n2.4 Parental leave: 12 weeks paid at 100% salary',\n",
       "  'chunk_id': 12},\n",
       " {'section': '1. Professional Standards',\n",
       "  'content': '• 1.1 Maintain highest ethical standards -\\n1.2 Report conflicts of interest -\\n1.3 Comply with all applicable laws and regulations -\\n1.4 Protect company and customer confidential information',\n",
       "  'chunk_id': 13},\n",
       " {'section': '2. Workplace Safety',\n",
       "  'content': '• 2.1 Report safety concerns immediately -\\n2.2 Follow all security protocols -\\n2.3 Zero tolerance for workplace violence -\\n2.4 Maintain required certifications',\n",
       "  'chunk_id': 14},\n",
       " {'section': '1. This Handbook is not a contract of employment and does not create contractual obligations of any',\n",
       "  'content': '1.1 kind between DeepShield and any employee.',\n",
       "  'chunk_id': 15},\n",
       " {'section': '3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may',\n",
       "  'content': '3.1 terminate the employment relationship at any time, with or without cause or notice.',\n",
       "  'chunk_id': 16},\n",
       " {'section': '8. Execution',\n",
       "  'content': '8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.\\nEmployee Name: _\\nEmployee Signature: Date: _\\n8.2 Authorized by: /s/ Dr. Marcus Chen\\n8.3 Chief Executive Officer\\n8.4 DeepShield Systems, Inc.\\n8.5 Date: January 1, 2023',\n",
       "  'chunk_id': 17}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_document_by_sections(cleaned_text, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd43a5",
   "metadata": {},
   "source": [
    "Has an issue where newlines are treated as new sections. Easy to see on Section 1 compared to section 1.1 which are both part of the same sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf62d7d",
   "metadata": {},
   "source": [
    "# Chunking using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7dc1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentChunker:\n",
    "    \"\"\"\n",
    "    Specialized chunker for legal documents that preserves section structure\n",
    "    while ensuring chunks fit within LLM context limits.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_chunk_size: int = 1000,\n",
    "                 chunk_overlap: int = 200,\n",
    "                 preserve_section_hierarchy: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the legal document chunker.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum characters per chunk\n",
    "            chunk_overlap: Number of characters to overlap between chunks\n",
    "            preserve_section_hierarchy: Whether to maintain section context in metadata\n",
    "        \"\"\"\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.preserve_section_hierarchy = preserve_section_hierarchy\n",
    "        \n",
    "        # Configure the recursive splitter for oversized sections\n",
    "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def convert_to_markdown_headers(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert numbered sections to markdown headers for better splitting.\n",
    "        \"\"\"\n",
    "        # Convert main sections (1. Title) to ## headers\n",
    "        text = re.sub(r'^(\\d+)\\.\\s+([A-Z][^\\n]+)', r'## \\1. \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Convert subsections (1.1 Title) to ### headers  \n",
    "        text = re.sub(r'^(\\d+\\.\\d+)\\.?\\s+([A-Z][^\\n]+)', r'### \\1 \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Convert sub-subsections (1.1.1 Title) to #### headers\n",
    "        text = re.sub(r'^(\\d+\\.\\d+\\.\\d+)\\.?\\s+([A-Z][^\\n]+)', r'#### \\1 \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def chunk_legal_document(self, cleaned_text: str, document_title: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk a legal document while preserving section structure.\n",
    "        \n",
    "        Args:\n",
    "            cleaned_text: Text that has been cleaned with clean_text_for_rag()\n",
    "            document_title: Optional document title for metadata\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing chunk text and metadata\n",
    "        \"\"\"\n",
    "        # Convert sections to markdown headers\n",
    "        markdown_text = self.convert_to_markdown_headers(cleaned_text)\n",
    "        \n",
    "        # Define headers to split on\n",
    "        headers_to_split_on = [\n",
    "            (\"##\", \"section\"),\n",
    "            (\"###\", \"subsection\"), \n",
    "            (\"####\", \"subsubsection\")\n",
    "        ]\n",
    "        \n",
    "        # Create markdown header splitter\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on\n",
    "        )\n",
    "        \n",
    "        # Split by headers first\n",
    "        header_chunks = markdown_splitter.split_text(markdown_text)\n",
    "        \n",
    "        # Process each chunk\n",
    "        final_chunks = []\n",
    "        \n",
    "        for chunk in header_chunks:\n",
    "            chunk_text = chunk.page_content\n",
    "            chunk_metadata = chunk.metadata.copy()\n",
    "            \n",
    "            # Add document-level metadata\n",
    "            if document_title:\n",
    "                chunk_metadata[\"document_title\"] = document_title\n",
    "            \n",
    "            # Check if chunk is too large and needs further splitting\n",
    "            if len(chunk_text) > self.max_chunk_size:\n",
    "                # Use recursive splitter for oversized chunks\n",
    "                sub_chunks = self.recursive_splitter.split_text(chunk_text)\n",
    "                \n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    sub_metadata = chunk_metadata.copy()\n",
    "                    sub_metadata[\"chunk_part\"] = f\"{i+1}/{len(sub_chunks)}\"\n",
    "                    sub_metadata[\"is_split_chunk\"] = True\n",
    "                    \n",
    "                    final_chunks.append({\n",
    "                        \"text\": sub_chunk,\n",
    "                        \"metadata\": sub_metadata,\n",
    "                        \"chunk_size\": len(sub_chunk)\n",
    "                    })\n",
    "            else:\n",
    "                chunk_metadata[\"is_split_chunk\"] = False\n",
    "                final_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": chunk_metadata,\n",
    "                    \"chunk_size\": len(chunk_text)\n",
    "                })\n",
    "        \n",
    "        return final_chunks\n",
    "    \n",
    "    def create_section_hierarchy_context(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Add hierarchical context to chunks for better retrieval.\n",
    "        \"\"\"\n",
    "        if not self.preserve_section_hierarchy:\n",
    "            return chunks\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            metadata = chunk[\"metadata\"]\n",
    "            context_parts = []\n",
    "            \n",
    "            # Build hierarchical context\n",
    "            if \"section\" in metadata:\n",
    "                context_parts.append(f\"Section: {metadata['section']}\")\n",
    "            if \"subsection\" in metadata:\n",
    "                context_parts.append(f\"Subsection: {metadata['subsection']}\")\n",
    "            if \"subsubsection\" in metadata:\n",
    "                context_parts.append(f\"Sub-subsection: {metadata['subsubsection']}\")\n",
    "            \n",
    "            if context_parts:\n",
    "                chunk[\"metadata\"][\"section_hierarchy\"] = \" > \".join(context_parts)\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c40427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_legal_document(pdf_text: str, document_title: str = None, debug: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: clean text and chunk for RAG system.\n",
    "    \n",
    "    Args:\n",
    "        pdf_text: Raw PDF text\n",
    "        document_title: Title of the document\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        List of chunks ready for embedding and vector storage\n",
    "    \"\"\"\n",
    "    # First clean the text (using the function from the previous artifact)\n",
    "    cleaned_text = clean_text_for_rag(pdf_text, debug=debug)\n",
    "    \n",
    "    # Then chunk it\n",
    "    chunker = LegalDocumentChunker(\n",
    "        max_chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        preserve_section_hierarchy=True\n",
    "    )\n",
    "    \n",
    "    chunks = chunker.chunk_legal_document(cleaned_text, document_title)\n",
    "    chunks = chunker.create_section_hierarchy_context(chunks)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Chunking complete:\")\n",
    "        print(f\"[DEBUG] Total chunks: {len(chunks)}\")\n",
    "        print(f\"[DEBUG] Average chunk size: {sum(c['chunk_size'] for c in chunks) / len(chunks):.0f} characters\")\n",
    "        print(f\"[DEBUG] Chunk size range: {min(c['chunk_size'] for c in chunks)} - {max(c['chunk_size'] for c in chunks)} characters\")\n",
    "        \n",
    "        # Show first few chunks\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\n[DEBUG] Chunk {i+1} metadata: {chunk['metadata']}\")\n",
    "            print(f\"[DEBUG] Chunk {i+1} preview: {chunk['text'][:200]}...\")\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b868bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting text cleaning. Original length: 4636 characters\n",
      "[DEBUG] Original text preview: DeepShield Systems Employee Handbook 2023\n",
      "1. Introduction and Purpose\n",
      "1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\n",
      "1.1 conditions applicable to all employees...\n",
      "[DEBUG] Step 1: Removed 0 page markers\n",
      "[DEBUG] Step 2: Removed 0 formatting artifact lines\n",
      "[DEBUG] Step 3: Removed 0 extra spaces\n",
      "[DEBUG] Step 4: Found 25 sections, 43 subsections, 0 sub-subsections\n",
      "[DEBUG] Step 5a: Normalized 0 multiple newline sequences\n",
      "[DEBUG] Step 5b: Joined 1 lines into paragraphs while preserving structure\n",
      "[DEBUG] Found main section: 1. Introduction and Purpose\n",
      "[DEBUG] Found main section: 1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\n",
      "[DEBUG] Found main section: 2. This Handbook supersedes all previous employee handbooks and management memos. The\n",
      "[DEBUG] Found main section: 2. Employment Policies\n",
      "[DEBUG] Found main section: 1. Equal Employment Opportunity\n",
      "[DEBUG] Found main section: 2. Employment Classification\n",
      "[DEBUG] Found main section: 3. Security Clearance Requirements\n",
      "[DEBUG] Found main section: 3. Workplace Policies\n",
      "[DEBUG] Found main section: 1. Confidentiality and Intellectual Property\n",
      "[DEBUG] Found main section: 2. Cybersecurity Compliance\n",
      "[DEBUG] Found main section: 3. Remote Work Policy\n",
      "[DEBUG] Found main section: 4. Compensation and Benefits\n",
      "[DEBUG] Found main section: 1. Compensation\n",
      "[DEBUG] Found main section: 2. Benefits Package\n",
      "[DEBUG] Found main section: 5. Time Off and Leave Policies\n",
      "[DEBUG] Found main section: 1. Paid Time Off (PTO)\n",
      "[DEBUG] Found main section: 2. Other Leave\n",
      "[DEBUG] Found main section: 6. Code of Conduct\n",
      "[DEBUG] Found main section: 1. Professional Standards\n",
      "[DEBUG] Found main section: 2. Workplace Safety\n",
      "[DEBUG] Found main section: 7. Acknowledgment and Disclaimer\n",
      "[DEBUG] Found main section: 1. This Handbook is not a contract of employment and does not create contractual obligations of any\n",
      "[DEBUG] Found main section: 2. DeepShield reserves the right to modify, revoke, suspend, terminate, or change any guidelines, 2.1 policies, or procedures, in whole or in part, at any time, with or without notice.\n",
      "[DEBUG] Found main section: 3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may\n",
      "[DEBUG] Found main section: 8. Execution\n",
      "[DEBUG] Auto-numbered paragraph as subsection 8.1: 'Employee Signature: Date: _'\n",
      "[DEBUG] Step 6: Standardized 11 bullet points\n",
      "[DEBUG] Step 7: Fixed spacing for 25 section numbers\n",
      "[DEBUG] Step 8: Fixed 0 multiple periods and normalized punctuation spacing\n",
      "[DEBUG] Step 9: Final cleanup - removed 0 excess empty lines, 0 extra spaces\n",
      "[DEBUG] Cleaning complete. Final length: 4641 characters\n",
      "[DEBUG] Size reduction: -5 characters (-0.1%)\n",
      "[DEBUG] Final text preview: DeepShield Systems Employee Handbook 2023\n",
      "1. Introduction and Purpose\n",
      "1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\n",
      "1.1 conditions applicable to all employees...\n",
      "\n",
      "[DEBUG] Chunking complete:\n",
      "[DEBUG] Total chunks: 16\n",
      "[DEBUG] Average chunk size: 72 characters\n",
      "[DEBUG] Chunk size range: 16 - 208 characters\n",
      "\n",
      "[DEBUG] Chunk 1 metadata: {'document_title': 'Sample Legal Document', 'is_split_chunk': False}\n",
      "[DEBUG] Chunk 1 preview: DeepShield Systems Employee Handbook 2023...\n",
      "\n",
      "[DEBUG] Chunk 2 metadata: {'section': '1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working'}\n",
      "[DEBUG] Chunk 2 preview: 1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, D...\n",
      "\n",
      "[DEBUG] Chunk 3 metadata: {'section': '2. This Handbook supersedes all previous employee handbooks and management memos. The', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. This Handbook supersedes all previous employee handbooks and management memos. The'}\n",
      "[DEBUG] Chunk 3 preview: 2.1 policies contained herein are effective as of January 1, 2023....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'DeepShield Systems Employee Handbook 2023',\n",
       "  'metadata': {'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False},\n",
       "  'chunk_size': 41},\n",
       " {'text': '1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.',\n",
       "  'metadata': {'section': '1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working'},\n",
       "  'chunk_size': 208},\n",
       " {'text': '2.1 policies contained herein are effective as of January 1, 2023.',\n",
       "  'metadata': {'section': '2. This Handbook supersedes all previous employee handbooks and management memos. The',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. This Handbook supersedes all previous employee handbooks and management memos. The'},\n",
       "  'chunk_size': 66},\n",
       " {'text': '• 2.1 Full-time Regular: Employees scheduled to work 40 hours per week -',\n",
       "  'metadata': {'section': '2. Employment Classification',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Employment Classification'},\n",
       "  'chunk_size': 72},\n",
       " {'text': \"• 1.1 All employees must sign and comply with the Company's Confidentiality and Intellectual\\nProperty Agreement •\",\n",
       "  'metadata': {'section': '1. Confidentiality and Intellectual Property',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Confidentiality and Intellectual Property'},\n",
       "  'chunk_size': 113},\n",
       " {'text': '• 2.1 Mandatory participation in quarterly cybersecurity training -',\n",
       "  'metadata': {'section': '2. Cybersecurity Compliance',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Cybersecurity Compliance'},\n",
       "  'chunk_size': 67},\n",
       " {'text': '• 3.1 Eligible employees may work remotely subject to manager approval -',\n",
       "  'metadata': {'section': '3. Remote Work Policy',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 3. Remote Work Policy'},\n",
       "  'chunk_size': 72},\n",
       " {'text': '• 1.1 Pay periods: Semi-monthly on the 15th and last day of each month -',\n",
       "  'metadata': {'section': '1. Compensation',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Compensation'},\n",
       "  'chunk_size': 72},\n",
       " {'text': '• 2.1 Medical, dental, and vision insurance - 401(k) plan with company match up to 4% -',\n",
       "  'metadata': {'section': '2. Benefits Package',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Benefits Package'},\n",
       "  'chunk_size': 87},\n",
       " {'text': '• 0-2 years: 15 days annually\\n• 3-5 years: 20 days annually - 6+ years: 25 days annually -',\n",
       "  'metadata': {'section': '1. Paid Time Off (PTO)',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Paid Time Off (PTO)'},\n",
       "  'chunk_size': 90},\n",
       " {'text': '• 2.1 Bereavement leave: Up to 5 days -',\n",
       "  'metadata': {'section': '2. Other Leave',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Other Leave'},\n",
       "  'chunk_size': 39},\n",
       " {'text': '• 1.1 Maintain highest ethical standards -',\n",
       "  'metadata': {'section': '1. Professional Standards',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Professional Standards'},\n",
       "  'chunk_size': 42},\n",
       " {'text': '• 2.1 Report safety concerns immediately -',\n",
       "  'metadata': {'section': '2. Workplace Safety',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Workplace Safety'},\n",
       "  'chunk_size': 42},\n",
       " {'text': '1.1 kind between DeepShield and any employee.',\n",
       "  'metadata': {'section': '1. This Handbook is not a contract of employment and does not create contractual obligations of any',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. This Handbook is not a contract of employment and does not create contractual obligations of any'},\n",
       "  'chunk_size': 45},\n",
       " {'text': '3.1 terminate the employment relationship at any time, with or without cause or notice.',\n",
       "  'metadata': {'section': '3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may'},\n",
       "  'chunk_size': 87},\n",
       " {'text': 'Employee Name: _',\n",
       "  'metadata': {'section': '8. Execution',\n",
       "   'subsection': '8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 8. Execution > Subsection: 8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.'},\n",
       "  'chunk_size': 16}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = process_legal_document(cleaned_text, document_title=\"Sample Legal Document\", debug=True)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9345d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.',\n",
       " 'metadata': {'section': '1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working',\n",
       "  'document_title': 'Sample Legal Document',\n",
       "  'is_split_chunk': False,\n",
       "  'section_hierarchy': 'Section: 1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working'},\n",
       " 'chunk_size': 208}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90111a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'document_title': 'Sample Legal Document', 'is_split_chunk': False}, page_content='DeepShield Systems Employee Handbook 2023'),\n",
       " Document(metadata={'section': '1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working'}, page_content='1.1 conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.'),\n",
       " Document(metadata={'section': '2. This Handbook supersedes all previous employee handbooks and management memos. The', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. This Handbook supersedes all previous employee handbooks and management memos. The'}, page_content='2.1 policies contained herein are effective as of January 1, 2023.'),\n",
       " Document(metadata={'section': '2. Employment Classification', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Employment Classification'}, page_content='• 2.1 Full-time Regular: Employees scheduled to work 40 hours per week -'),\n",
       " Document(metadata={'section': '1. Confidentiality and Intellectual Property', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Confidentiality and Intellectual Property'}, page_content=\"• 1.1 All employees must sign and comply with the Company's Confidentiality and Intellectual\\nProperty Agreement •\"),\n",
       " Document(metadata={'section': '2. Cybersecurity Compliance', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Cybersecurity Compliance'}, page_content='• 2.1 Mandatory participation in quarterly cybersecurity training -'),\n",
       " Document(metadata={'section': '3. Remote Work Policy', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Remote Work Policy'}, page_content='• 3.1 Eligible employees may work remotely subject to manager approval -'),\n",
       " Document(metadata={'section': '1. Compensation', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Compensation'}, page_content='• 1.1 Pay periods: Semi-monthly on the 15th and last day of each month -'),\n",
       " Document(metadata={'section': '2. Benefits Package', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Benefits Package'}, page_content='• 2.1 Medical, dental, and vision insurance - 401(k) plan with company match up to 4% -'),\n",
       " Document(metadata={'section': '1. Paid Time Off (PTO)', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Paid Time Off (PTO)'}, page_content='• 0-2 years: 15 days annually\\n• 3-5 years: 20 days annually - 6+ years: 25 days annually -'),\n",
       " Document(metadata={'section': '2. Other Leave', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Other Leave'}, page_content='• 2.1 Bereavement leave: Up to 5 days -'),\n",
       " Document(metadata={'section': '1. Professional Standards', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Professional Standards'}, page_content='• 1.1 Maintain highest ethical standards -'),\n",
       " Document(metadata={'section': '2. Workplace Safety', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Workplace Safety'}, page_content='• 2.1 Report safety concerns immediately -'),\n",
       " Document(metadata={'section': '1. This Handbook is not a contract of employment and does not create contractual obligations of any', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. This Handbook is not a contract of employment and does not create contractual obligations of any'}, page_content='1.1 kind between DeepShield and any employee.'),\n",
       " Document(metadata={'section': '3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Employment at DeepShield is \"at-will, \" meaning either the employee or DeepShield may'}, page_content='3.1 terminate the employment relationship at any time, with or without cause or notice.'),\n",
       " Document(metadata={'section': '8. Execution', 'subsection': '8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 8. Execution > Subsection: 8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.'}, page_content='Employee Name: _')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integration with LangChain vector stores\n",
    "def create_langchain_documents(chunks: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Convert chunks to LangChain Document objects for vector store ingestion.\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        doc = Document(\n",
    "            page_content=chunk[\"text\"],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "langchian_docs = create_langchain_documents(chunks)\n",
    "langchian_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eff903",
   "metadata": {},
   "source": [
    "Same issue in LangChain chunking that treats a newline as a seperate chunk. Seems to be an issue with the scanning/cleaning parts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
