{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075ea9e2",
   "metadata": {},
   "source": [
    "# Amazon Textract\n",
    "\n",
    "Testing Amazon Textract but mostly having SubscriptionRequired errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31362e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from botocore.exceptions import ClientError, NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ba38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Textract Client Function\n",
    "def initialize_textract_client(region_name: str = 'us-east-1', debug: bool = False) -> boto3.client:\n",
    "    \"\"\"\n",
    "    Initialize AWS Textract client \n",
    "    \n",
    "    Args:\n",
    "        region_name (str): AWS region for Textract service\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        boto3.client: Configured Textract client\n",
    "    \n",
    "    Raises:\n",
    "        NoCredentialsError: If AWS credentials are not configured\n",
    "        Exception: For other AWS configuration issues\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Initializing Textract client for region: {region_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the Textract client\n",
    "        textract = boto3.client('textract', region_name=region_name)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"[DEBUG] Successfully created Textract client\")\n",
    "            # Test client by listing available operations (doesn't make API call)\n",
    "            operations = [op for op in dir(textract) if not op.startswith('_') and callable(getattr(textract, op))][:5]\n",
    "            print(f\"[DEBUG] Available operations sample: {operations}\")\n",
    "        \n",
    "        return textract\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        error_msg = \"AWS credentials not found. Please configure AWS credentials.\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to initialize Textract client: {str(e)}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25e61a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initializing Textract client for region: us-east-1\n",
      "[DEBUG] Successfully created Textract client\n",
      "[DEBUG] Available operations sample: ['analyze_document', 'analyze_expense', 'analyze_id', 'can_paginate', 'close']\n",
      "‚úÖ Textract client initialized successfully!\n",
      "Service name: textract\n",
      "Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Test Textract Client Initialization\n",
    "try:\n",
    "    # Test the function with debug enabled\n",
    "    textract_client = initialize_textract_client(debug=True)\n",
    "    print(\"‚úÖ Textract client initialized successfully!\")\n",
    "    print(f\"Service name: {textract_client._service_model.service_name}\")\n",
    "    print(f\"Region: {textract_client.meta.region_name}\")\n",
    "    \n",
    "except NoCredentialsError:\n",
    "    print(\"‚ùå AWS credentials not configured. Please set up your AWS credentials.\")\n",
    "    print(\"You can configure them using:\")\n",
    "    print(\"1. AWS CLI: aws configure\")\n",
    "    print(\"2. Environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "    print(\"3. IAM roles (if running on EC2)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Textract client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Directory Scanner Function\n",
    "def scan_pdf_directory(root_directory: str, debug: bool = False) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively scan directory for PDF files and return structured information.\n",
    "    \n",
    "    Args:\n",
    "        root_directory (str): Path to root directory containing PDFs\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of PDF file information with keys: 'path', 'name', 'size', 'subfolder'\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Scanning directory: {root_directory}\")\n",
    "    \n",
    "    pdf_files = []\n",
    "    root_path = Path(root_directory)\n",
    "    \n",
    "    if not root_path.exists():\n",
    "        error_msg = f\"Directory does not exist: {root_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        return pdf_files\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        error_msg = f\"Path is not a directory: {root_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "        return pdf_files\n",
    "    \n",
    "    # Recursively find all PDF files\n",
    "    pdf_pattern = \"**/*.pdf\"\n",
    "    found_files = list(root_path.glob(pdf_pattern))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Found {len(found_files)} PDF files\")\n",
    "    \n",
    "    for pdf_path in found_files:\n",
    "        try:\n",
    "            # Get file stats\n",
    "            file_stats = pdf_path.stat()\n",
    "            \n",
    "            # Determine subfolder relative to root\n",
    "            relative_path = pdf_path.relative_to(root_path)\n",
    "            subfolder = str(relative_path.parent) if relative_path.parent != Path('.') else 'root'\n",
    "            \n",
    "            pdf_info = {\n",
    "                'path': str(pdf_path.absolute()),\n",
    "                'name': pdf_path.name,\n",
    "                'size': file_stats.st_size,\n",
    "                'subfolder': subfolder,\n",
    "                'size_mb': round(file_stats.st_size / (1024 * 1024), 2)\n",
    "            }\n",
    "            \n",
    "            pdf_files.append(pdf_info)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found PDF: {pdf_info['name']} ({pdf_info['size_mb']} MB) in {subfolder}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing file {pdf_path}: {str(e)}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    # Sort by subfolder then by name\n",
    "    pdf_files.sort(key=lambda x: (x['subfolder'], x['name']))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Successfully processed {len(pdf_files)} PDF files\")\n",
    "        \n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfaf17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for PDF files...\n",
      "[DEBUG] Scanning directory: ./pdfs\n",
      "[DEBUG] Found 1 PDF files\n",
      "[DEBUG] Found PDF: deepshield-systems-employee-handbook-2023.pdf (0.01 MB) in root\n",
      "[DEBUG] Successfully processed 1 PDF files\n",
      "\n",
      " Summary:\n",
      "Total PDFs found: 1\n",
      "Total size: 0.01 MB\n",
      "Subfolders: 1\n",
      "\n",
      "üìÅ Files by subfolder:\n",
      "  root: 1 files (0.01 MB)\n",
      "    - deepshield-systems-employee-handbook-2023.pdf (0.01 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Directory Scanner\n",
    "pdf_directory = \"./pdfs\"  # Change this to your actual directory path if needed\n",
    "\n",
    "print(\"Scanning for PDF files...\")\n",
    "pdf_files = scan_pdf_directory(pdf_directory, debug=True)\n",
    "\n",
    "print(f\"\\n Summary:\")\n",
    "print(f\"Total PDFs found: {len(pdf_files)}\")\n",
    "\n",
    "if pdf_files:\n",
    "    # Group by subfolder\n",
    "    subfolders = {}\n",
    "    total_size = 0\n",
    "    \n",
    "    for pdf in pdf_files:\n",
    "        subfolder = pdf['subfolder']\n",
    "        if subfolder not in subfolders:\n",
    "            subfolders[subfolder] = []\n",
    "        subfolders[subfolder].append(pdf)\n",
    "        total_size += pdf['size']\n",
    "    \n",
    "    print(f\"Total size: {round(total_size / (1024 * 1024), 2)} MB\")\n",
    "    print(f\"Subfolders: {len(subfolders)}\")\n",
    "    \n",
    "    print(\"\\nüìÅ Files by subfolder:\")\n",
    "    for subfolder, files in subfolders.items():\n",
    "        folder_size = sum(f['size'] for f in files)\n",
    "        print(f\"  {subfolder}: {len(files)} files ({round(folder_size / (1024 * 1024), 2)} MB)\")\n",
    "        \n",
    "        # Show first few files as examples\n",
    "        for i, file in enumerate(files[:3]):\n",
    "            print(f\"    - {file['name']} ({file['size_mb']} MB)\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"    ... and {len(files) - 3} more\")\n",
    "else:\n",
    "    print(\"‚ùå No PDF files found. Please check the directory path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a5f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Text Extraction Function using Textract\n",
    "def extract_text_from_pdf(pdf_path: str, textract_client: boto3.client, debug: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF using AWS Textract (synchronous processing).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        textract_client (boto3.client): Initialized Textract client\n",
    "        debug (bool): Enable debug logging\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Extraction results with keys: 'success', 'text', 'confidence', 'error', 'page_count'\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Processing PDF: {pdf_path}\")\n",
    "    \n",
    "    result = {\n",
    "        'success': False,\n",
    "        'text': '',\n",
    "        'confidence': 0.0,\n",
    "        'error': None,\n",
    "        'page_count': 0,\n",
    "        'file_size_mb': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        pdf_file = Path(pdf_path)\n",
    "        if not pdf_file.exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = pdf_file.stat().st_size\n",
    "        result['file_size_mb'] = round(file_size / (1024 * 1024), 2)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] File size: {result['file_size_mb']} MB\")\n",
    "        \n",
    "        # Check file size limit (10MB for synchronous processing)\n",
    "        if file_size > 10 * 1024 * 1024:\n",
    "            raise ValueError(f\"File too large for synchronous processing: {result['file_size_mb']} MB (max 10MB)\")\n",
    "        \n",
    "        # Read the PDF file\n",
    "        with open(pdf_path, 'rb') as document:\n",
    "            document_bytes = document.read()\n",
    "        \n",
    "        if debug:\n",
    "            print(\"[DEBUG] File read successfully, calling Textract...\")\n",
    "        \n",
    "        # Call Textract\n",
    "        response = textract_client.detect_document_text(\n",
    "            Document={'Bytes': document_bytes}\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Textract response received with {len(response.get('Blocks', []))} blocks\")\n",
    "        \n",
    "        # Extract text from blocks\n",
    "        extracted_text = []\n",
    "        confidences = []\n",
    "        \n",
    "        for block in response.get('Blocks', []):\n",
    "            if block['BlockType'] == 'LINE':\n",
    "                text = block.get('Text', '')\n",
    "                confidence = block.get('Confidence', 0)\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                confidences.append(confidence)\n",
    "                \n",
    "                if debug and len(extracted_text) <= 3:  # Show first few lines\n",
    "                    print(f\"[DEBUG] Line: '{text}' (confidence: {confidence:.1f}%)\")\n",
    "        \n",
    "        # Combine results\n",
    "        result['text'] = '\\n'.join(extracted_text)\n",
    "        result['confidence'] = sum(confidences) / len(confidences) if confidences else 0\n",
    "        result['page_count'] = len([block for block in response.get('Blocks', []) if block['BlockType'] == 'PAGE'])\n",
    "        result['success'] = True\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Extraction successful!\")\n",
    "            print(f\"[DEBUG] Pages: {result['page_count']}\")\n",
    "            print(f\"[DEBUG] Lines extracted: {len(extracted_text)}\")\n",
    "            print(f\"[DEBUG] Average confidence: {result['confidence']:.1f}%\")\n",
    "            print(f\"[DEBUG] Text length: {len(result['text'])} characters\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = f\"AWS Textract error ({error_code}): {e.response['Error']['Message']}\"\n",
    "        result['error'] = error_msg\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing PDF: {str(e)}\"\n",
    "        result['error'] = error_msg\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63f42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing PDF text extraction...\n",
      "\n",
      "üìÑ Test 1/1: deepshield-systems-employee-handbook-2023.pdf\n",
      "   Size: 0.01 MB | Subfolder: root\n",
      "[DEBUG] Processing PDF: c:\\Code\\Code for learning at Diligence\\Text Chunking and Processing\\PDF-Processing-Pipeline\\pdfs\\deepshield-systems-employee-handbook-2023.pdf\n",
      "[DEBUG] File size: 0.01 MB\n",
      "[DEBUG] File read successfully, calling Textract...\n",
      "[DEBUG] AWS Textract error (SubscriptionRequiredException): The AWS Access Key Id needs a subscription for the service\n",
      "   ‚ùå Failed: AWS Textract error (SubscriptionRequiredException): The AWS Access Key Id needs a subscription for the service\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìà Test Summary:\n",
      "Files tested: 1\n",
      "Files suitable for sync processing: 1\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Text Extraction\n",
    "# First ensure we have a working Textract client\n",
    "try:\n",
    "    if 'textract_client' not in locals():\n",
    "        print(\"üîÑ Initializing Textract client...\")\n",
    "        textract_client = initialize_textract_client(debug=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Textract client: {e}\")\n",
    "    textract_client = None\n",
    "\n",
    "if 'pdf_files' in locals() and pdf_files and textract_client:\n",
    "    print(\"üß™ Testing PDF text extraction...\\n\")\n",
    "    \n",
    "    # Test on first few PDFs (or all if less than 3)\n",
    "    test_files = pdf_files[:min(3, len(pdf_files))]\n",
    "    \n",
    "    for i, pdf_info in enumerate(test_files, 1):\n",
    "        print(f\"üìÑ Test {i}/{len(test_files)}: {pdf_info['name']}\")\n",
    "        print(f\"   Size: {pdf_info['size_mb']} MB | Subfolder: {pdf_info['subfolder']}\")\n",
    "        \n",
    "        # Skip if file is too large for sync processing\n",
    "        if pdf_info['size_mb'] > 10:\n",
    "            print(\"   ‚ö†Ô∏è  Skipping - File too large for synchronous processing (>10MB)\")\n",
    "            continue\n",
    "        \n",
    "        # Extract text\n",
    "        result = extract_text_from_pdf(pdf_info['path'], textract_client, debug=True)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"   ‚úÖ Success!\")\n",
    "            print(f\"   üìä Pages: {result['page_count']} | Confidence: {result['confidence']:.1f}%\")\n",
    "            print(f\"   üìù Text length: {len(result['text'])} characters\")\n",
    "            \n",
    "            # Show first 200 characters of extracted text\n",
    "            preview_text = result['text'][:200].replace('\\n', ' ')\n",
    "            print(f\"   üîç Preview: {preview_text}...\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed: {result['error']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Summary\n",
    "    successful_extractions = sum(1 for pdf in test_files if pdf['size_mb'] <= 10)\n",
    "    print(f\"\\nüìà Test Summary:\")\n",
    "    print(f\"Files tested: {len(test_files)}\")\n",
    "    print(f\"Files suitable for sync processing: {successful_extractions}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run test - missing PDF files or Textract client\")\n",
    "    print(\"Please run the previous cells first to scan for PDFs and initialize the client.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d6d15",
   "metadata": {},
   "source": [
    "# PDF Extraction Uses PyMuPDF\n",
    "\n",
    "Since Textract isn't working, trying with PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9ee3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6607187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyMuPDF for PDF Extraction\n",
    "def extract_text_from_pdf(pdf_path, debug=False):\n",
    "    \"\"\"\n",
    "    Extract text from a single PDF file\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text from the PDF\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting text extraction for: {pdf_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF document\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] PDF opened successfully. Total pages: {doc.page_count}\")\n",
    "        \n",
    "        # Iterate through each page\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            # Extract text from the page\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            if debug:\n",
    "                char_count = len(page_text.strip())\n",
    "                print(f\"[DEBUG] Page {page_num + 1}: Extracted {char_count} characters\")\n",
    "            \n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "            text += page_text\n",
    "            text += \"\\n\"\n",
    "        \n",
    "        # Close the document\n",
    "        doc.close()\n",
    "        \n",
    "        if debug:\n",
    "            total_chars = len(text.strip())\n",
    "            print(f\"[DEBUG] Completed extraction. Total characters: {total_chars}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error extracting text from {pdf_path}: {str(e)}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return \"\"\n",
    "\n",
    "# Find PDF files in a directory (Don't care about file size unlike other function)\n",
    "def find_pdf_files(directory, debug=False):\n",
    "    \"\"\"\n",
    "    Find all PDF files in directory and subdirectories\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Root directory to search\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        list: List of PDF file paths\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Searching for PDF files in: {directory}\")\n",
    "    \n",
    "    pdf_files = []\n",
    "    directory_path = Path(directory)\n",
    "    \n",
    "    if not directory_path.exists():\n",
    "        error_msg = f\"Directory {directory} does not exist\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return []\n",
    "    \n",
    "    # Recursively find all PDF files\n",
    "    for pdf_file in directory_path.rglob(\"*.pdf\"):\n",
    "        if pdf_file.is_file():\n",
    "            pdf_files.append(str(pdf_file))\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found PDF: {pdf_file}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total PDF files found: {len(pdf_files)}\")\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "def extract_pdfs_from_directory(input_directory, output_file=None, save_individual=False, debug=False):\n",
    "    \"\"\"\n",
    "    Extract text from all PDFs in a directory and its subdirectories\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Directory containing PDF files\n",
    "        output_file (str, optional): Path to save combined extracted text\n",
    "        save_individual (bool): Whether to save each PDF's text individually\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with PDF paths as keys and extracted text as values\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting PDF extraction from directory: {input_directory}\")\n",
    "        print(f\"[DEBUG] Output file: {output_file}\")\n",
    "        print(f\"[DEBUG] Save individual files: {save_individual}\")\n",
    "    \n",
    "    if not os.path.exists(input_directory):\n",
    "        error_msg = f\"Directory {input_directory} does not exist\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        return {}\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = find_pdf_files(input_directory, debug=debug)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        warning_msg = f\"No PDF files found in {input_directory}\"\n",
    "        if debug:\n",
    "            print(f\"[DEBUG WARNING] {warning_msg}\")\n",
    "        else:\n",
    "            print(warning_msg)\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    extracted_texts = {}\n",
    "    all_text = \"\"\n",
    "    successful_extractions = 0\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Processing file {i}/{len(pdf_files)}: {pdf_path}\")\n",
    "        else:\n",
    "            print(f\"Processing ({i}/{len(pdf_files)}): {os.path.basename(pdf_path)}\")\n",
    "        \n",
    "        # Extract text from PDF\n",
    "        text = extract_text_from_pdf(pdf_path, debug=debug)\n",
    "        \n",
    "        if text.strip():  # Check if there's actual content\n",
    "            extracted_texts[pdf_path] = text\n",
    "            successful_extractions += 1\n",
    "            \n",
    "            # Add to combined text\n",
    "            all_text += f\"\\n{'='*50}\\n\"\n",
    "            all_text += f\"FILE: {pdf_path}\\n\"\n",
    "            all_text += f\"{'='*50}\\n\"\n",
    "            all_text += text\n",
    "            all_text += \"\\n\"\n",
    "            \n",
    "            # Save individual file if requested\n",
    "            if save_individual:\n",
    "                output_path = pdf_path.replace('.pdf', '_extracted.txt')\n",
    "                try:\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(text)\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Saved individual text to: {output_path}\")\n",
    "                    else:\n",
    "                        print(f\"  ‚Üí Saved individual text file\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error saving individual file {output_path}: {str(e)}\"\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "                    else:\n",
    "                        print(f\"  ‚Üí Error saving individual file: {str(e)}\")\n",
    "        \n",
    "        else:\n",
    "            warning_msg = f\"No text extracted from: {pdf_path}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG WARNING] {warning_msg}\")\n",
    "            else:\n",
    "                print(f\"  ‚Üí No text extracted\")\n",
    "    \n",
    "    # Save combined text if output file specified\n",
    "    if output_file and all_text:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(all_text)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Saved combined text to: {output_file}\")\n",
    "            else:\n",
    "                print(f\"Saved combined text to: {output_file}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error saving combined file {output_file}: {str(e)}\"\n",
    "            if debug:\n",
    "                print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "            else:\n",
    "                print(f\"Error saving combined file: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nExtraction complete!\")\n",
    "    print(f\"Successfully processed {successful_extractions}/{len(pdf_files)} PDF files\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total characters in combined text: {len(all_text)}\")\n",
    "        print(f\"[DEBUG] Extraction results dictionary has {len(extracted_texts)} entries\")\n",
    "    \n",
    "    return extracted_texts\n",
    "\n",
    "def simple_extract(directory_path, debug=False):\n",
    "    \"\"\"\n",
    "    Simple function to extract text from all PDFs in a directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing PDFs\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with PDF paths as keys and extracted text as values\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Simple extract called for directory: {directory_path}\")\n",
    "    \n",
    "    return extract_pdfs_from_directory(directory_path, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3330b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files\n",
      "Processing (1/1): deepshield-systems-employee-handbook-2023.pdf\n",
      "\n",
      "Extraction complete!\n",
      "Successfully processed 1/1 PDF files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdfs\\\\deepshield-systems-employee-handbook-2023.pdf': '\\n--- Page 1 ---\\nDeepShield Systems Employee Handbook 2023\\n1. Introduction and Purpose\\n1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working\\nconditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the\\n\"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400,\\nWilmington, DE 19801.\\n2. This Handbook supersedes all previous employee handbooks and management memos. The\\npolicies contained herein are effective as of January 1, 2023.\\n2. Employment Policies\\n1. Equal Employment Opportunity\\nDeepShield provides equal employment opportunities to all employees and applicants without regard\\nto race, color, religion, sex, national origin, age, disability, genetic information, or any other\\nprotected characteristic.\\n2. Employment Classification\\n-\\nFull-time Regular: Employees scheduled to work 40 hours per week\\n-\\nPart-time Regular: Employees scheduled to work less than 40 hours per week\\n-\\nTemporary: Employees hired for a specific project or time period\\n-\\nExempt: Salaried employees exempt from overtime requirements\\n-\\nNon-exempt: Employees eligible for overtime compensation\\n3. Security Clearance Requirements\\nGiven the nature of DeepShield\\'s work in critical infrastructure protection, certain positions may\\nrequire government security clearances. Employees must maintain required clearances as a condition\\nof employment.\\n3. Workplace Policies\\n1. Confidentiality and Intellectual Property\\n-\\nAll employees must sign and comply with the Company\\'s Confidentiality and Intellectual\\nProperty Agreement\\n\\n\\n--- Page 2 ---\\n-\\nProprietary information includes but is not limited to: source code, security architectures,\\ncustomer data, threat detection algorithms, and maritime protection protocols\\n-\\nEmployees must report any potential security breaches or unauthorized access immediately\\n2. Cybersecurity Compliance\\n-\\nMandatory participation in quarterly cybersecurity training\\n-\\nStrict adherence to the Company\\'s \"Clean Screen\" policy\\n-\\nTwo-factor authentication required for all system access\\n-\\nProhibition on use of unauthorized software or devices\\n3. Remote Work Policy\\n-\\nEligible employees may work remotely subject to manager approval\\n-\\nRemote work must comply with DeepShield\\'s security protocols\\n-\\nVPN and encrypted connections required for all remote access\\n-\\nRegular office presence required for collaborative projects\\n4. Compensation and Benefits\\n1. Compensation\\n-\\nPay periods: Semi-monthly on the 15th and last day of each month\\n-\\nPerformance reviews conducted annually\\n-\\nMerit increases based on performance and market conditions\\n-\\nEquity compensation available per individual agreements\\n2. Benefits Package\\n-\\nMedical, dental, and vision insurance\\n-\\n401(k) plan with company match up to 4%\\n-\\nLife and disability insurance\\n-\\nProfessional development allowance ($5,000 annually)\\n-\\nSecurity certification reimbursement program\\n5. Time Off and Leave Policies\\n1. Paid Time Off (PTO)\\n-\\n0-2 years: 15 days annually\\n\\n\\n--- Page 3 ---\\n-\\n3-5 years: 20 days annually\\n-\\n6+ years: 25 days annually\\n-\\nMaximum accrual cap: 1.5x annual allowance\\n2. Other Leave\\n-\\nBereavement leave: Up to 5 days\\n-\\nJury duty: Paid as required by law\\n-\\nMilitary leave: In accordance with USERRA\\n-\\nParental leave: 12 weeks paid at 100% salary\\n6. Code of Conduct\\n1. Professional Standards\\n-\\nMaintain highest ethical standards\\n-\\nReport conflicts of interest\\n-\\nComply with all applicable laws and regulations\\n-\\nProtect company and customer confidential information\\n2. Workplace Safety\\n-\\nReport safety concerns immediately\\n-\\nFollow all security protocols\\n-\\nZero tolerance for workplace violence\\n-\\nMaintain required certifications\\n7. Acknowledgment and Disclaimer\\n1. This Handbook is not a contract of employment and does not create contractual obligations of any\\nkind between DeepShield and any employee.\\n2. DeepShield reserves the right to modify, revoke, suspend, terminate, or change any guidelines,\\npolicies, or procedures, in whole or in part, at any time, with or without notice.\\n3. Employment at DeepShield is \"at-will,\" meaning either the employee or DeepShield may\\nterminate the employment relationship at any time, with or without cause or notice.\\n8. Execution\\n\\n\\n--- Page 4 ---\\nI acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply\\nwith its contents.\\nEmployee Name: _\\nEmployee Signature: \\nDate: _\\nAuthorized by:\\n/s/ Dr. Marcus Chen\\nChief Executive Officer\\nDeepShield Systems, Inc.\\nDate: January 1, 2023\\n\\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_texts = simple_extract(r\".\\pdfs\")\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232b2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Text Extraction Strategy using PyMuPDF block based extraction\n",
    "def extract_text_with_layout(pdf_path: str, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from PDF using PyMuPDF's block-based extraction for better paragraph detection.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        debug (bool): Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text with better paragraph preservation\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting layout-aware extraction for: {pdf_path}\")\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] PDF opened successfully. Total pages: {doc.page_count}\")\n",
    "        \n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # Use block-based extraction which preserves paragraph structure\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Page {page_num + 1}: Found {len(blocks)} text blocks\")\n",
    "            \n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "            \n",
    "            # Process blocks - each block is typically a paragraph or text unit\n",
    "            for block_num, block in enumerate(blocks):\n",
    "                # block format: (x0, y0, x1, y1, \"text\", block_no, block_type)\n",
    "                # block_type 0 = text, 1 = image\n",
    "                if block[6] == 0:  # Text block\n",
    "                    block_text = block[4]\n",
    "                    \n",
    "                    # Clean up the block text\n",
    "                    # Join lines within the block that are clearly continuations\n",
    "                    lines = block_text.split('\\n')\n",
    "                    cleaned_lines = []\n",
    "                    \n",
    "                    for i, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if this line should be joined with the previous one\n",
    "                        if cleaned_lines and not line[0].isupper() and not line[0].isdigit():\n",
    "                            # Likely a continuation - join with previous line\n",
    "                            cleaned_lines[-1] += \" \" + line\n",
    "                        elif cleaned_lines and not cleaned_lines[-1].endswith(('.', ':', '-', ';')):\n",
    "                            # Previous line doesn't end with punctuation - likely continuation\n",
    "                            cleaned_lines[-1] += \" \" + line\n",
    "                        else:\n",
    "                            # New line/paragraph\n",
    "                            cleaned_lines.append(line)\n",
    "                    \n",
    "                    # Join the cleaned lines within this block\n",
    "                    block_text = ' '.join(cleaned_lines)\n",
    "                    \n",
    "                    if block_text.strip():\n",
    "                        text += block_text + \"\\n\\n\"\n",
    "                        \n",
    "                        if debug and block_num < 3:\n",
    "                            preview = block_text[:100].replace('\\n', ' ')\n",
    "                            print(f\"[DEBUG]   Block {block_num}: {preview}...\")\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Extraction complete. Total characters: {len(text)}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error extracting text from {pdf_path}: {str(e)}\"\n",
    "        print(f\"[DEBUG ERROR] {error_msg}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c02f86",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2605c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_rag(text: str, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Improved text cleaning with hierarchical section structure.\n",
    "    Uses positional logic instead of keywords for generalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting improved text cleaning. Original length: {len(text)} characters\")\n",
    "    \n",
    "    original_text = text\n",
    "    \n",
    "    # Steps 1-3: Same as before (page markers, artifacts, whitespace)\n",
    "    text = re.sub(r'--- Page \\d+ ---\\n?', '', text)\n",
    "    text = re.sub(r'^[-_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'_{3,}', '[SIGNATURE LINE]', text)\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Steps 1-3 complete: Basic cleaning\")\n",
    "    \n",
    "    # Step 4: Join line continuations\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 4: Joining line continuations\")\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    joined_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    def is_section_header(line: str) -> bool:\n",
    "        \"\"\"Detect if a line is a section header.\"\"\"\n",
    "        if not re.match(r'^\\d+\\.\\s+', line):\n",
    "            return False\n",
    "        if len(line) > 60:\n",
    "            return False\n",
    "        text_part = re.sub(r'^\\d+\\.\\s+', '', line)\n",
    "        if text_part.endswith(('.', '!', '?')):\n",
    "            return False\n",
    "        words = text_part.split()\n",
    "        if len(words) > 1:\n",
    "            capitalized = sum(1 for w in words if w and w[0].isupper())\n",
    "            if capitalized / len(words) > 0.5:\n",
    "                return True\n",
    "        if len(words) <= 3 and len(line) < 40:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    while i < len(lines):\n",
    "        current_line = lines[i].strip()\n",
    "        \n",
    "        is_bullet = re.match(r'^[-‚Ä¢*]\\s+', current_line)\n",
    "        is_signature = '[SIGNATURE LINE]' in current_line\n",
    "        is_header = is_section_header(current_line)\n",
    "        \n",
    "        if is_bullet or is_signature or is_header:\n",
    "            joined_lines.append(current_line)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        paragraph = [current_line]\n",
    "        i += 1\n",
    "        \n",
    "        while i < len(lines):\n",
    "            next_line = lines[i].strip()\n",
    "            \n",
    "            if (re.match(r'^[-‚Ä¢*]\\s+', next_line) or \n",
    "                '[SIGNATURE LINE]' in next_line or \n",
    "                is_section_header(next_line)):\n",
    "                break\n",
    "            \n",
    "            prev_line = paragraph[-1]\n",
    "            first_char = next_line[0] if next_line else ''\n",
    "            starts_lowercase = first_char.isalpha() and first_char.islower()\n",
    "            \n",
    "            prev_no_end_punct = not prev_line.endswith(('.', '!', '?', ':', ';'))\n",
    "            prev_has_continuation = prev_line.rstrip().endswith((',', 'and', 'or', 'the', 'of', 'to', 'in', 'for', 'with', 'at', 'as', 'by'))\n",
    "            \n",
    "            if starts_lowercase or prev_no_end_punct or prev_has_continuation:\n",
    "                if first_char.isdigit() and len(next_line) > 60:\n",
    "                    break\n",
    "                paragraph.append(next_line)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        joined_lines.append(' '.join(paragraph))\n",
    "    \n",
    "    text = '\\n'.join(joined_lines)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 4 complete\")\n",
    "    \n",
    "    # Step 5: Detect hierarchy based on position\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 5: Detecting hierarchy from position\")\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    structured_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        if not line_stripped or not is_section_header(line_stripped):\n",
    "            structured_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Check what came before this header\n",
    "        prev_idx = i - 1\n",
    "        while prev_idx >= 0 and not lines[prev_idx].strip():\n",
    "            prev_idx -= 1\n",
    "        \n",
    "        # Determine if main section or subsection\n",
    "        if prev_idx < 0:\n",
    "            # First item in document - main section\n",
    "            structured_lines.append(line_stripped + \" [MAIN]\")\n",
    "        else:\n",
    "            prev_line = lines[prev_idx].strip()\n",
    "            prev_is_header = is_section_header(prev_line)\n",
    "            \n",
    "            if prev_is_header:\n",
    "                # Header following header = subsection\n",
    "                structured_lines.append(line_stripped + \" [SUB]\")\n",
    "            else:\n",
    "                # Header following content = new main section\n",
    "                structured_lines.append(line_stripped + \" [MAIN]\")\n",
    "        \n",
    "        if debug:\n",
    "            marker = \"[MAIN]\" if \"[MAIN]\" in structured_lines[-1] else \"[SUB]\"\n",
    "            print(f\"[DEBUG]   Line {i}: {marker} '{line_stripped}'\")\n",
    "    \n",
    "    text = '\\n'.join(structured_lines)\n",
    "    \n",
    "    # Step 6: Renumber based on hierarchy\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 6: Renumbering\")\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    current_main = None\n",
    "    sub_counter = 1\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        if not line_stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "        \n",
    "        if '[MAIN]' in line_stripped:\n",
    "            line_stripped = line_stripped.replace(' [MAIN]', '')\n",
    "            match = re.match(r'^(\\d+)\\.\\s+(.+)', line_stripped)\n",
    "            if match:\n",
    "                current_main = match.group(1)\n",
    "                sub_counter = 1\n",
    "                processed_lines.append('')\n",
    "                processed_lines.append(line_stripped)\n",
    "                processed_lines.append('')\n",
    "            continue\n",
    "        \n",
    "        if '[SUB]' in line_stripped:\n",
    "            line_stripped = line_stripped.replace(' [SUB]', '')\n",
    "            if current_main:\n",
    "                text_part = re.sub(r'^\\d+\\.\\s+', '', line_stripped)\n",
    "                new_line = f\"{current_main}.{sub_counter} {text_part}\"\n",
    "                processed_lines.append(new_line)\n",
    "                sub_counter += 1\n",
    "            else:\n",
    "                processed_lines.append(line_stripped)\n",
    "            continue\n",
    "        \n",
    "        # Keep everything else as-is (no auto-numbering of content)\n",
    "        processed_lines.append(line_stripped)\n",
    "    \n",
    "    text = '\\n'.join(processed_lines)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Step 6 complete\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    text = re.sub(r'^[-‚Ä¢*]\\s+', '‚Ä¢ ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\s*:\\s*', ': ', text)\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    prev_empty = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "            prev_empty = False\n",
    "        elif not prev_empty:\n",
    "            cleaned_lines.append('')\n",
    "            prev_empty = True\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines).strip()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Complete. Final length: {len(text)} characters\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09b7fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_rag(text: str, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Improved text cleaning with hierarchical section structure.\n",
    "    Detects main sections by observing when numbering resets to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Starting improved text cleaning. Original length: {len(text)} characters\")\n",
    "    \n",
    "    original_text = text\n",
    "    \n",
    "    # Steps 1-3: Basic cleaning\n",
    "    text = re.sub(r'--- Page \\d+ ---\\n?', '', text)\n",
    "    text = re.sub(r'^[-_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'_{3,}', '[SIGNATURE LINE]', text)\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    # Step 4: Join line continuations\n",
    "    lines = text.split('\\n')\n",
    "    joined_lines = []\n",
    "    i = 0\n",
    "    \n",
    "    def is_section_header(line: str) -> bool:\n",
    "        \"\"\"Detect if a line is a section header.\"\"\"\n",
    "        if not re.match(r'^\\d+\\.\\s+', line):\n",
    "            return False\n",
    "        if len(line) > 60:\n",
    "            return False\n",
    "        text_part = re.sub(r'^\\d+\\.\\s+', '', line)\n",
    "        if text_part.endswith(('.', '!', '?')):\n",
    "            return False\n",
    "        words = text_part.split()\n",
    "        if len(words) > 1:\n",
    "            capitalized = sum(1 for w in words if w and w[0].isupper())\n",
    "            if capitalized / len(words) > 0.5:\n",
    "                return True\n",
    "        if len(words) <= 3 and len(line) < 40:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    while i < len(lines):\n",
    "        current_line = lines[i].strip()\n",
    "        \n",
    "        is_bullet = re.match(r'^[-‚Ä¢*]\\s+', current_line)\n",
    "        is_signature = '[SIGNATURE LINE]' in current_line\n",
    "        is_header = is_section_header(current_line)\n",
    "        \n",
    "        if is_bullet or is_signature or is_header:\n",
    "            joined_lines.append(current_line)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        paragraph = [current_line]\n",
    "        i += 1\n",
    "        \n",
    "        while i < len(lines):\n",
    "            next_line = lines[i].strip()\n",
    "            \n",
    "            if (re.match(r'^[-‚Ä¢*]\\s+', next_line) or \n",
    "                '[SIGNATURE LINE]' in next_line or \n",
    "                is_section_header(next_line)):\n",
    "                break\n",
    "            \n",
    "            prev_line = paragraph[-1]\n",
    "            first_char = next_line[0] if next_line else ''\n",
    "            starts_lowercase = first_char.isalpha() and first_char.islower()\n",
    "            \n",
    "            prev_no_end_punct = not prev_line.endswith(('.', '!', '?', ':', ';'))\n",
    "            prev_has_continuation = prev_line.rstrip().endswith((',', 'and', 'or', 'the', 'of', 'to', 'in', 'for', 'with', 'at', 'as', 'by'))\n",
    "            \n",
    "            if starts_lowercase or prev_no_end_punct or prev_has_continuation:\n",
    "                if first_char.isdigit() and len(next_line) > 60:\n",
    "                    break\n",
    "                paragraph.append(next_line)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        joined_lines.append(' '.join(paragraph))\n",
    "    \n",
    "    text = '\\n'.join(joined_lines)\n",
    "    \n",
    "    # Step 5: Detect headers and classify using reset-to-1 logic\n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Step 5: Detecting headers with reset-to-1 logic\")\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # First pass: find all headers\n",
    "    header_positions = []\n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        if is_section_header(line_stripped):\n",
    "            num_match = re.match(r'^(\\d+)\\.', line_stripped)\n",
    "            number = int(num_match.group(1)) if num_match else 0\n",
    "            header_positions.append({\n",
    "                'index': i,\n",
    "                'line': line_stripped,\n",
    "                'number': number\n",
    "            })\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Found {len(header_positions)} headers\")\n",
    "        print(f\"[DEBUG] Numbers: {[h['number'] for h in header_positions]}\")\n",
    "    \n",
    "    # Second pass: classify based on reset-to-1 pattern\n",
    "    # A header that is numbered \"1\" AFTER we've seen other numbers indicates\n",
    "    # the START of a subsection group under a new main section\n",
    "    structured_lines = []\n",
    "    header_idx = 0\n",
    "    current_main_num = 0\n",
    "    in_subsection_group = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        if header_idx < len(header_positions) and i == header_positions[header_idx]['index']:\n",
    "            current_header = header_positions[header_idx]\n",
    "            current_num = current_header['number']\n",
    "            \n",
    "            is_main = False\n",
    "            \n",
    "            if header_idx == 0:\n",
    "                # First header\n",
    "                if current_num == 1:\n",
    "                    # Starts with 1, it's main\n",
    "                    is_main = True\n",
    "                    current_main_num = 1\n",
    "                    in_subsection_group = False\n",
    "                else:\n",
    "                    # Starts with non-1, treat as main anyway\n",
    "                    is_main = True\n",
    "                    current_main_num = current_num\n",
    "                    in_subsection_group = False\n",
    "            else:\n",
    "                prev_num = header_positions[header_idx - 1]['number']\n",
    "                \n",
    "                # Check if this is a reset to 1 after being in a sequence\n",
    "                if current_num == 1 and prev_num > 1:\n",
    "                    # This marks the start of a subsection group\n",
    "                    # The PREVIOUS high number was actually a main section\n",
    "                    # We need to retroactively fix this\n",
    "                    in_subsection_group = True\n",
    "                    is_main = False\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Header {header_idx}: Detected reset to 1 (after {prev_num}), entering subsection group\")\n",
    "                elif in_subsection_group:\n",
    "                    # We're in a subsection group, check if we should exit\n",
    "                    if current_num > prev_num + 1:\n",
    "                        # Big jump, this is a new main section\n",
    "                        is_main = True\n",
    "                        current_main_num = current_num\n",
    "                        in_subsection_group = False\n",
    "                        if debug:\n",
    "                            print(f\"[DEBUG] Header {header_idx}: Jump from {prev_num} to {current_num}, exiting subsection group\")\n",
    "                    else:\n",
    "                        # Continue in subsection group\n",
    "                        is_main = False\n",
    "                else:\n",
    "                    # Not in subsection group\n",
    "                    # If we see a sequential increase, stay as main\n",
    "                    if current_num == prev_num + 1 or current_num == prev_num:\n",
    "                        # Might be continuing main or starting subsection\n",
    "                        # Check ahead: if next header is 1, current is main\n",
    "                        if header_idx + 1 < len(header_positions):\n",
    "                            next_num = header_positions[header_idx + 1]['number']\n",
    "                            if next_num == 1:\n",
    "                                # Next is 1, so current is main\n",
    "                                is_main = True\n",
    "                                current_main_num = current_num\n",
    "                            else:\n",
    "                                # Unclear, default to main if sequential\n",
    "                                is_main = True\n",
    "                                current_main_num = current_num\n",
    "                        else:\n",
    "                            is_main = True\n",
    "                            current_main_num = current_num\n",
    "                    else:\n",
    "                        is_main = True\n",
    "                        current_main_num = current_num\n",
    "            \n",
    "            if is_main:\n",
    "                structured_lines.append(line_stripped + \" [MAIN]\")\n",
    "            else:\n",
    "                structured_lines.append(line_stripped + \" [SUB]\")\n",
    "            \n",
    "            header_idx += 1\n",
    "        else:\n",
    "            structured_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(structured_lines)\n",
    "    \n",
    "    # Step 6: Renumber\n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Step 6: Renumbering\")\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    current_main = None\n",
    "    sub_counter = 1\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        if not line_stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "        \n",
    "        if '[MAIN]' in line_stripped:\n",
    "            line_stripped = line_stripped.replace(' [MAIN]', '')\n",
    "            match = re.match(r'^(\\d+)\\.\\s+(.+)', line_stripped)\n",
    "            if match:\n",
    "                current_main = match.group(1)\n",
    "                sub_counter = 1\n",
    "                processed_lines.append('')\n",
    "                processed_lines.append(line_stripped)\n",
    "                processed_lines.append('')\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG]   Main {current_main}: '{line_stripped}'\")\n",
    "            continue\n",
    "        \n",
    "        if '[SUB]' in line_stripped:\n",
    "            line_stripped = line_stripped.replace(' [SUB]', '')\n",
    "            if current_main:\n",
    "                text_part = re.sub(r'^\\d+\\.\\s+', '', line_stripped)\n",
    "                new_line = f\"{current_main}.{sub_counter} {text_part}\"\n",
    "                processed_lines.append(new_line)\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG]   Sub {current_main}.{sub_counter}: '{text_part}'\")\n",
    "                sub_counter += 1\n",
    "            else:\n",
    "                processed_lines.append(line_stripped)\n",
    "            continue\n",
    "        \n",
    "        processed_lines.append(line_stripped)\n",
    "    \n",
    "    text = '\\n'.join(processed_lines)\n",
    "    \n",
    "    # Final cleanup\n",
    "    text = re.sub(r'^[-‚Ä¢*]\\s+', '‚Ä¢ ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\s*:\\s*', ': ', text)\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    prev_empty = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "            prev_empty = False\n",
    "        elif not prev_empty:\n",
    "            cleaned_lines.append('')\n",
    "            prev_empty = True\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines).strip()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Complete\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eeefbf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting improved text cleaning. Original length: 4511 characters\n",
      "\n",
      "[DEBUG] Step 5: Detecting headers with reset-to-1 logic\n",
      "[DEBUG] Found 20 headers\n",
      "[DEBUG] Numbers: [1, 2, 1, 2, 3, 3, 1, 2, 3, 4, 1, 2, 5, 1, 2, 6, 1, 2, 7, 8]\n",
      "[DEBUG] Header 2: Detected reset to 1 (after 2), entering subsection group\n",
      "[DEBUG] Header 6: Detected reset to 1 (after 3), entering subsection group\n",
      "[DEBUG] Header 10: Detected reset to 1 (after 4), entering subsection group\n",
      "[DEBUG] Header 12: Jump from 2 to 5, exiting subsection group\n",
      "[DEBUG] Header 13: Detected reset to 1 (after 5), entering subsection group\n",
      "[DEBUG] Header 15: Jump from 2 to 6, exiting subsection group\n",
      "[DEBUG] Header 16: Detected reset to 1 (after 6), entering subsection group\n",
      "[DEBUG] Header 18: Jump from 2 to 7, exiting subsection group\n",
      "\n",
      "[DEBUG] Step 6: Renumbering\n",
      "[DEBUG]   Main 1: '1. Introduction and Purpose'\n",
      "[DEBUG]   Main 2: '2. Employment Policies'\n",
      "[DEBUG]   Sub 2.1: 'Equal Employment Opportunity'\n",
      "[DEBUG]   Sub 2.2: 'Employment Classification'\n",
      "[DEBUG]   Sub 2.3: 'Security Clearance Requirements'\n",
      "[DEBUG]   Sub 2.4: 'Workplace Policies'\n",
      "[DEBUG]   Sub 2.5: 'Confidentiality and Intellectual Property'\n",
      "[DEBUG]   Sub 2.6: 'Cybersecurity Compliance'\n",
      "[DEBUG]   Sub 2.7: 'Remote Work Policy'\n",
      "[DEBUG]   Sub 2.8: 'Compensation and Benefits'\n",
      "[DEBUG]   Sub 2.9: 'Compensation'\n",
      "[DEBUG]   Sub 2.10: 'Benefits Package'\n",
      "[DEBUG]   Main 5: '5. Time Off and Leave Policies'\n",
      "[DEBUG]   Sub 5.1: 'Paid Time Off (PTO)'\n",
      "[DEBUG]   Sub 5.2: 'Other Leave'\n",
      "[DEBUG]   Main 6: '6. Code of Conduct'\n",
      "[DEBUG]   Sub 6.1: 'Professional Standards'\n",
      "[DEBUG]   Sub 6.2: 'Workplace Safety'\n",
      "[DEBUG]   Main 7: '7. Acknowledgment and Disclaimer'\n",
      "[DEBUG]   Main 8: '8. Execution'\n",
      "\n",
      "[DEBUG] Complete\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = clean_text_for_rag(next(iter(extracted_texts.values())), debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d10ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepShield Systems Employee Handbook 2023\n",
      "\n",
      "1. Introduction and Purpose\n",
      "\n",
      "1. This Employee Handbook (\"Handbook\") sets forth the policies, procedures, and working conditions applicable to all employees of DeepShield Systems, Inc. (\"DeepShield\" or the \"Company\"), a Delaware corporation with principal offices at 2100 Innovation Drive, Suite 400, Wilmington, DE 19801.\n",
      "2. This Handbook supersedes all previous employee handbooks and management memos. The policies contained herein are effective as of January 1, 2023.\n",
      "\n",
      "2. Employment Policies\n",
      "\n",
      "2.1 Equal Employment Opportunity\n",
      "DeepShield provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, genetic information, or any other protected characteristic.\n",
      "2.2 Employment Classification\n",
      "‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or time period - Exempt: Salaried employees exempt from overtime requirements - Non-exempt: Employees eligible for overtime compensation\n",
      "2.3 Security Clearance Requirements\n",
      "Given the nature of DeepShield's work in critical infrastructure protection, certain positions may require government security clearances. Employees must maintain required clearances as a condition of employment.\n",
      "2.4 Workplace Policies\n",
      "2.5 Confidentiality and Intellectual Property\n",
      "‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols - Employees must report any potential security breaches or unauthorized access immediately\n",
      "2.6 Cybersecurity Compliance\n",
      "‚Ä¢ Mandatory participation in quarterly cybersecurity training - Strict adherence to the Company's \"Clean Screen\" policy - Two-factor authentication required for all system access - Prohibition on use of unauthorized software or devices\n",
      "2.7 Remote Work Policy\n",
      "‚Ä¢ Eligible employees may work remotely subject to manager approval - Remote work must comply with DeepShield's security protocols - VPN and encrypted connections required for all remote access - Regular office presence required for collaborative projects\n",
      "2.8 Compensation and Benefits\n",
      "2.9 Compensation\n",
      "‚Ä¢ Pay periods: Semi-monthly on the 15th and last day of each month - Performance reviews conducted annually - Merit increases based on performance and market conditions - Equity compensation available per individual agreements\n",
      "2.10 Benefits Package\n",
      "‚Ä¢ Medical, dental, and vision insurance - 401(k) plan with company match up to 4% - Life and disability insurance - Professional development allowance ($5,000 annually) - Security certification reimbursement program\n",
      "\n",
      "5. Time Off and Leave Policies\n",
      "\n",
      "5.1 Paid Time Off (PTO)\n",
      "‚Ä¢ 0-2 years: 15 days annually - 3-5 years: 20 days annually - 6+ years: 25 days annually - Maximum accrual cap: 1.5x annual allowance\n",
      "5.2 Other Leave\n",
      "‚Ä¢ Bereavement leave: Up to 5 days - Jury duty: Paid as required by law - Military leave: In accordance with USERRA - Parental leave: 12 weeks paid at 100% salary\n",
      "\n",
      "6. Code of Conduct\n",
      "\n",
      "6.1 Professional Standards\n",
      "‚Ä¢ Maintain highest ethical standards - Report conflicts of interest - Comply with all applicable laws and regulations - Protect company and customer confidential information\n",
      "6.2 Workplace Safety\n",
      "‚Ä¢ Report safety concerns immediately - Follow all security protocols - Zero tolerance for workplace violence - Maintain required certifications\n",
      "\n",
      "7. Acknowledgment and Disclaimer\n",
      "\n",
      "1. This Handbook is not a contract of employment and does not create contractual obligations of any kind between DeepShield and any employee.\n",
      "2. DeepShield reserves the right to modify, revoke, suspend, terminate, or change any guidelines, policies, or procedures, in whole or in part, at any time, with or without notice.\n",
      "3. Employment at DeepShield is \"at-will,\" meaning either the employee or DeepShield may terminate the employment relationship at any time, with or without cause or notice.\n",
      "\n",
      "8. Execution\n",
      "\n",
      "I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.\n",
      "Employee Name: _ Employee Signature: Date: _ Authorized by: /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.\n",
      "Date: January 1, 2023\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb50876",
   "metadata": {},
   "source": [
    "# Text Chunking not using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac0861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk by section headers\n",
    "def chunk_document_by_sections(cleaned_text: str, debug: bool = False) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split the cleaned document into logical chunks based on sections.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_text (str): Cleaned text from clean_text_for_rag()\n",
    "        debug (bool): If True, prints debug information about chunking process\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Starting document chunking\")\n",
    "        print(f\"[DEBUG] Input text length: {len(cleaned_text)} characters\")\n",
    "        print(f\"[DEBUG] Input text lines: {len(cleaned_text.splitlines())}\")\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # First, let's see what section headers we can find\n",
    "    section_headers = re.findall(r'^(\\d+\\.\\s+[^\\n]+)', cleaned_text, flags=re.MULTILINE)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Found potential section headers:\")\n",
    "        for i, header in enumerate(section_headers):\n",
    "            print(f\"[DEBUG]   {i+1}: '{header}'\")\n",
    "    \n",
    "    # Split by major sections (numbered sections like \"1. Introduction\")\n",
    "    # Use a more flexible pattern that captures the full header line\n",
    "    sections = re.split(r'^(\\d+\\.\\s+[^\\n]*)', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Split resulted in {len(sections)} segments\")\n",
    "        for i, segment in enumerate(sections[:6]):  # Show first 6 segments\n",
    "            segment_preview = segment.strip()[:100].replace('\\n', '\\\\n')\n",
    "            print(f\"[DEBUG]   Segment {i}: '{segment_preview}...'\")\n",
    "    \n",
    "    current_section = \"Document Header\"\n",
    "    current_content = \"\"\n",
    "    segment_count = 0\n",
    "    \n",
    "    for i, segment in enumerate(sections):\n",
    "        segment = segment.strip()\n",
    "        \n",
    "        if not segment:  # Skip empty segments\n",
    "            continue\n",
    "            \n",
    "        # Check if this segment is a section header\n",
    "        if re.match(r'^\\d+\\.\\s+[A-Za-z]', segment):\n",
    "            # This is a section header - save previous section if it has content\n",
    "            if current_content.strip():\n",
    "                chunk_length = len(current_content.strip())\n",
    "                chunks.append({\n",
    "                    'section': current_section,\n",
    "                    'content': current_content.strip(),\n",
    "                    'chunk_id': len(chunks)\n",
    "                })\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Created chunk {len(chunks)-1}: '{current_section}' ({chunk_length} chars)\")\n",
    "            \n",
    "            # Start new section\n",
    "            current_section = segment\n",
    "            current_content = \"\"\n",
    "            segment_count += 1\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Starting new section: '{current_section}'\")\n",
    "        else:\n",
    "            # This is section content - add to current section\n",
    "            if current_content:\n",
    "                current_content += \"\\n\\n\" + segment\n",
    "            else:\n",
    "                current_content = segment\n",
    "            if debug and len(segment) > 50:\n",
    "                print(f\"[DEBUG] Added content to '{current_section[:30]}...': {len(segment)} chars\")\n",
    "    \n",
    "    # Add the last section if it has content\n",
    "    if current_content.strip():\n",
    "        chunk_length = len(current_content.strip())\n",
    "        chunks.append({\n",
    "            'section': current_section,\n",
    "            'content': current_content.strip(),\n",
    "            'chunk_id': len(chunks)\n",
    "        })\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Created final chunk {len(chunks)-1}: '{current_section}' ({chunk_length} chars)\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Chunking summary:\")\n",
    "        print(f\"[DEBUG] - Found {len(section_headers)} section headers in text\")\n",
    "        print(f\"[DEBUG] - Processed {segment_count} sections\")\n",
    "        print(f\"[DEBUG] - Created {len(chunks)} chunks\")\n",
    "        \n",
    "        total_content_length = sum(len(chunk['content']) for chunk in chunks)\n",
    "        avg_chunk_length = total_content_length / len(chunks) if chunks else 0\n",
    "        print(f\"[DEBUG] - Total content length: {total_content_length} characters\")\n",
    "        print(f\"[DEBUG] - Average chunk length: {avg_chunk_length:.1f} characters\")\n",
    "        \n",
    "        # Show chunk size distribution\n",
    "        chunk_sizes = [len(chunk['content']) for chunk in chunks]\n",
    "        if chunk_sizes:\n",
    "            print(f\"[DEBUG] - Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "            \n",
    "        # Show all chunk sections\n",
    "        print(f\"[DEBUG] Final chunks created:\")\n",
    "        for chunk in chunks:\n",
    "            print(f\"[DEBUG]   Chunk {chunk['chunk_id']}: '{chunk['section'][:60]}...' ({len(chunk['content'])} chars)\")\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6939488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'section': 'Document Header',\n",
       "  'content': 'DeepShield Systems Employee Handbook 2023',\n",
       "  'chunk_id': 0},\n",
       " {'section': '2. Employment Policies',\n",
       "  'content': '2.1 Equal Employment Opportunity\\n2.1.1 DeepShield provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, national origin, age, disability, genetic information, or any other protected characteristic.',\n",
       "  'chunk_id': 1},\n",
       " {'section': '2. Employment Classification',\n",
       "  'content': '‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or time period - Exempt: Salaried employees exempt from overtime requirements - Non-exempt: Employees eligible for overtime compensation',\n",
       "  'chunk_id': 2},\n",
       " {'section': '3. Security Clearance Requirements',\n",
       "  'content': \"3.1 Given the nature of DeepShield's work in critical infrastructure protection, certain positions may require government security clearances. Employees must maintain required clearances as a condition of employment.\",\n",
       "  'chunk_id': 3},\n",
       " {'section': '3. Workplace Policies',\n",
       "  'content': \"3.1 Confidentiality and Intellectual Property\\n‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols - Employees must report any potential security breaches or unauthorized access immediately\",\n",
       "  'chunk_id': 4},\n",
       " {'section': '2. Cybersecurity Compliance',\n",
       "  'content': '‚Ä¢ Mandatory participation in quarterly cybersecurity training - Strict adherence to the Company\\'s \"Clean Screen\" policy - Two-factor authentication required for all system access - Prohibition on use of unauthorized software or devices',\n",
       "  'chunk_id': 5},\n",
       " {'section': '3. Remote Work Policy',\n",
       "  'content': \"‚Ä¢ Eligible employees may work remotely subject to manager approval - Remote work must comply with DeepShield's security protocols - VPN and encrypted connections required for all remote access - Regular office presence required for collaborative projects\",\n",
       "  'chunk_id': 6},\n",
       " {'section': '1. Compensation',\n",
       "  'content': '‚Ä¢ Pay periods: Semi-monthly on the 15th and last day of each month - Performance reviews conducted annually - Merit increases based on performance and market conditions - Equity compensation available per individual agreements',\n",
       "  'chunk_id': 7},\n",
       " {'section': '2. Benefits Package',\n",
       "  'content': '‚Ä¢ Medical, dental, and vision insurance - 401(k) plan with company match up to 4% - Life and disability insurance - Professional development allowance ($5, 000 annually) - Security certification reimbursement program',\n",
       "  'chunk_id': 8},\n",
       " {'section': '1. Paid Time Off (PTO)',\n",
       "  'content': '‚Ä¢ 0-2 years: 15 days annually - 3-5 years: 20 days annually - 6+ years: 25 days annually - Maximum accrual cap: 1.5x annual allowance',\n",
       "  'chunk_id': 9},\n",
       " {'section': '2. Other Leave',\n",
       "  'content': '‚Ä¢ Bereavement leave: Up to 5 days - Jury duty: Paid as required by law - Military leave: In accordance with USERRA - Parental leave: 12 weeks paid at 100% salary',\n",
       "  'chunk_id': 10},\n",
       " {'section': '6. Code of Conduct',\n",
       "  'content': '6.1 Professional Standards\\n‚Ä¢ Maintain highest ethical standards - Report conflicts of interest - Comply with all applicable laws and regulations - Protect company and customer confidential information',\n",
       "  'chunk_id': 11},\n",
       " {'section': '2. Workplace Safety',\n",
       "  'content': '‚Ä¢ Report safety concerns immediately - Follow all security protocols - Zero tolerance for workplace violence - Maintain required certifications',\n",
       "  'chunk_id': 12},\n",
       " {'section': '8. Execution',\n",
       "  'content': '8.1 I acknowledge receipt of the DeepShield Systems Employee Handbook and agree to read and comply with its contents.\\n8.2 Employee Name: _ Employee Signature: Date: _ Authorized by: 8.3 /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.\\nDate: January 1, 2023',\n",
       "  'chunk_id': 13}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_document_by_sections(cleaned_text, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd43a5",
   "metadata": {},
   "source": [
    "Has an issue where newlines are treated as new sections. Easy to see on Section 1 compared to section 1.1 which are both part of the same sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf62d7d",
   "metadata": {},
   "source": [
    "# Chunking using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7dc1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentChunker:\n",
    "    \"\"\"\n",
    "    Specialized chunker for legal documents that preserves section structure\n",
    "    while ensuring chunks fit within LLM context limits.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_chunk_size: int = 1000,\n",
    "                 chunk_overlap: int = 200,\n",
    "                 preserve_section_hierarchy: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the legal document chunker.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum characters per chunk\n",
    "            chunk_overlap: Number of characters to overlap between chunks\n",
    "            preserve_section_hierarchy: Whether to maintain section context in metadata\n",
    "        \"\"\"\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.preserve_section_hierarchy = preserve_section_hierarchy\n",
    "        \n",
    "        # Configure the recursive splitter for oversized sections\n",
    "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def convert_to_markdown_headers(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert numbered sections to markdown headers for better splitting.\n",
    "        \"\"\"\n",
    "        # Convert main sections (1. Title) to ## headers\n",
    "        text = re.sub(r'^(\\d+)\\.\\s+([A-Z][^\\n]+)', r'## \\1. \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Convert subsections (1.1 Title) to ### headers  \n",
    "        text = re.sub(r'^(\\d+\\.\\d+)\\.?\\s+([A-Z][^\\n]+)', r'### \\1 \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Convert sub-subsections (1.1.1 Title) to #### headers\n",
    "        text = re.sub(r'^(\\d+\\.\\d+\\.\\d+)\\.?\\s+([A-Z][^\\n]+)', r'#### \\1 \\2', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def chunk_legal_document(self, cleaned_text: str, document_title: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk a legal document while preserving section structure.\n",
    "        \n",
    "        Args:\n",
    "            cleaned_text: Text that has been cleaned with clean_text_for_rag()\n",
    "            document_title: Optional document title for metadata\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing chunk text and metadata\n",
    "        \"\"\"\n",
    "        # Convert sections to markdown headers\n",
    "        markdown_text = self.convert_to_markdown_headers(cleaned_text)\n",
    "        \n",
    "        # Define headers to split on\n",
    "        headers_to_split_on = [\n",
    "            (\"##\", \"section\"),\n",
    "            (\"###\", \"subsection\"), \n",
    "            (\"####\", \"subsubsection\")\n",
    "        ]\n",
    "        \n",
    "        # Create markdown header splitter\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on\n",
    "        )\n",
    "        \n",
    "        # Split by headers first\n",
    "        header_chunks = markdown_splitter.split_text(markdown_text)\n",
    "        \n",
    "        # Process each chunk\n",
    "        final_chunks = []\n",
    "        \n",
    "        for chunk in header_chunks:\n",
    "            chunk_text = chunk.page_content\n",
    "            chunk_metadata = chunk.metadata.copy()\n",
    "            \n",
    "            # Add document-level metadata\n",
    "            if document_title:\n",
    "                chunk_metadata[\"document_title\"] = document_title\n",
    "            \n",
    "            # Check if chunk is too large and needs further splitting\n",
    "            if len(chunk_text) > self.max_chunk_size:\n",
    "                # Use recursive splitter for oversized chunks\n",
    "                sub_chunks = self.recursive_splitter.split_text(chunk_text)\n",
    "                \n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    sub_metadata = chunk_metadata.copy()\n",
    "                    sub_metadata[\"chunk_part\"] = f\"{i+1}/{len(sub_chunks)}\"\n",
    "                    sub_metadata[\"is_split_chunk\"] = True\n",
    "                    \n",
    "                    final_chunks.append({\n",
    "                        \"text\": sub_chunk,\n",
    "                        \"metadata\": sub_metadata,\n",
    "                        \"chunk_size\": len(sub_chunk)\n",
    "                    })\n",
    "            else:\n",
    "                chunk_metadata[\"is_split_chunk\"] = False\n",
    "                final_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": chunk_metadata,\n",
    "                    \"chunk_size\": len(chunk_text)\n",
    "                })\n",
    "        \n",
    "        return final_chunks\n",
    "    \n",
    "    def create_section_hierarchy_context(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Add hierarchical context to chunks for better retrieval.\n",
    "        \"\"\"\n",
    "        if not self.preserve_section_hierarchy:\n",
    "            return chunks\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            metadata = chunk[\"metadata\"]\n",
    "            context_parts = []\n",
    "            \n",
    "            # Build hierarchical context\n",
    "            if \"section\" in metadata:\n",
    "                context_parts.append(f\"Section: {metadata['section']}\")\n",
    "            if \"subsection\" in metadata:\n",
    "                context_parts.append(f\"Subsection: {metadata['subsection']}\")\n",
    "            if \"subsubsection\" in metadata:\n",
    "                context_parts.append(f\"Sub-subsection: {metadata['subsubsection']}\")\n",
    "            \n",
    "            if context_parts:\n",
    "                chunk[\"metadata\"][\"section_hierarchy\"] = \" > \".join(context_parts)\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c40427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_legal_document(pdf_text: str, document_title: str = None, debug: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: clean text and chunk for RAG system.\n",
    "    \n",
    "    Args:\n",
    "        pdf_text: Raw PDF text\n",
    "        document_title: Title of the document\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        List of chunks ready for embedding and vector storage\n",
    "    \"\"\"\n",
    "    # First clean the text (using the function from the previous artifact)\n",
    "    cleaned_text = clean_text_for_rag(pdf_text, debug=debug)\n",
    "    \n",
    "    # Then chunk it\n",
    "    chunker = LegalDocumentChunker(\n",
    "        max_chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        preserve_section_hierarchy=True\n",
    "    )\n",
    "    \n",
    "    chunks = chunker.chunk_legal_document(cleaned_text, document_title)\n",
    "    chunks = chunker.create_section_hierarchy_context(chunks)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Chunking complete:\")\n",
    "        print(f\"[DEBUG] Total chunks: {len(chunks)}\")\n",
    "        print(f\"[DEBUG] Average chunk size: {sum(c['chunk_size'] for c in chunks) / len(chunks):.0f} characters\")\n",
    "        print(f\"[DEBUG] Chunk size range: {min(c['chunk_size'] for c in chunks)} - {max(c['chunk_size'] for c in chunks)} characters\")\n",
    "        \n",
    "        # Show first few chunks\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\n[DEBUG] Chunk {i+1} metadata: {chunk['metadata']}\")\n",
    "            print(f\"[DEBUG] Chunk {i+1} preview: {chunk['text'][:200]}...\")\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b868bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting improved text cleaning. Original length: 4500 characters\n",
      "[DEBUG] Step 1: Removed 0 page markers\n",
      "[DEBUG] Step 2: Cleaned formatting artifacts\n",
      "[DEBUG] Step 3: Normalized whitespace and filtered empty lines\n",
      "[DEBUG] Step 4: Starting intelligent line continuation joining\n",
      "[DEBUG] Step 4 complete: Joined 1 lines\n",
      "[DEBUG] Step 5: Identifying hierarchical structure\n",
      "[DEBUG] Step 6: Auto-numbering with hierarchy\n",
      "[DEBUG]   Main section 1: '1. Introduction and Purpose'\n",
      "[DEBUG]   Main section 2: '2. Employment Policies'\n",
      "[DEBUG]   Main section 2: '2. Employment Classification'\n",
      "[DEBUG]   Main section 3: '3. Security Clearance Requirements'\n",
      "[DEBUG]   Main section 3: '3. Workplace Policies'\n",
      "[DEBUG]   Main section 2: '2. Cybersecurity Compliance'\n",
      "[DEBUG]   Main section 3: '3. Remote Work Policy'\n",
      "[DEBUG]   Main section 4: '4. Compensation and Benefits'\n",
      "[DEBUG]   Main section 1: '1. Compensation'\n",
      "[DEBUG]   Main section 2: '2. Benefits Package'\n",
      "[DEBUG]   Main section 5: '5. Time Off and Leave Policies'\n",
      "[DEBUG]   Main section 1: '1. Paid Time Off (PTO)'\n",
      "[DEBUG]   Main section 2: '2. Other Leave'\n",
      "[DEBUG]   Main section 6: '6. Code of Conduct'\n",
      "[DEBUG]   Main section 2: '2. Workplace Safety'\n",
      "[DEBUG]   Main section 7: '7. Acknowledgment and Disclaimer'\n",
      "[DEBUG]   Main section 8: '8. Execution'\n",
      "[DEBUG] Step 6 complete: Auto-numbered 0 items\n",
      "[DEBUG] Cleaning complete. Final length: 4500 characters\n",
      "\n",
      "[DEBUG] Chunking complete:\n",
      "[DEBUG] Total chunks: 12\n",
      "[DEBUG] Average chunk size: 192 characters\n",
      "[DEBUG] Chunk size range: 21 - 371 characters\n",
      "\n",
      "[DEBUG] Chunk 1 metadata: {'document_title': 'Sample Legal Document', 'is_split_chunk': False}\n",
      "[DEBUG] Chunk 1 preview: DeepShield Systems Employee Handbook 2023...\n",
      "\n",
      "[DEBUG] Chunk 2 metadata: {'section': '2. Employment Classification', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Employment Classification'}\n",
      "[DEBUG] Chunk 2 preview: ‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or t...\n",
      "\n",
      "[DEBUG] Chunk 3 metadata: {'section': '3. Workplace Policies', 'subsection': '3.1 Confidentiality and Intellectual Property', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Workplace Policies > Subsection: 3.1 Confidentiality and Intellectual Property'}\n",
      "[DEBUG] Chunk 3 preview: ‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is not limited to: source code, security architecture...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'DeepShield Systems Employee Handbook 2023',\n",
       "  'metadata': {'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False},\n",
       "  'chunk_size': 41},\n",
       " {'text': '‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or time period - Exempt: Salaried employees exempt from overtime requirements - Non-exempt: Employees eligible for overtime compensation',\n",
       "  'metadata': {'section': '2. Employment Classification',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Employment Classification'},\n",
       "  'chunk_size': 332},\n",
       " {'text': \"‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols - Employees must report any potential security breaches or unauthorized access immediately\",\n",
       "  'metadata': {'section': '3. Workplace Policies',\n",
       "   'subsection': '3.1 Confidentiality and Intellectual Property',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 3. Workplace Policies > Subsection: 3.1 Confidentiality and Intellectual Property'},\n",
       "  'chunk_size': 371},\n",
       " {'text': '‚Ä¢ Mandatory participation in quarterly cybersecurity training - Strict adherence to the Company\\'s \"Clean Screen\" policy - Two-factor authentication required for all system access - Prohibition on use of unauthorized software or devices',\n",
       "  'metadata': {'section': '2. Cybersecurity Compliance',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Cybersecurity Compliance'},\n",
       "  'chunk_size': 235},\n",
       " {'text': \"‚Ä¢ Eligible employees may work remotely subject to manager approval - Remote work must comply with DeepShield's security protocols - VPN and encrypted connections required for all remote access - Regular office presence required for collaborative projects\",\n",
       "  'metadata': {'section': '3. Remote Work Policy',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 3. Remote Work Policy'},\n",
       "  'chunk_size': 254},\n",
       " {'text': '‚Ä¢ Pay periods: Semi-monthly on the 15th and last day of each month - Performance reviews conducted annually - Merit increases based on performance and market conditions - Equity compensation available per individual agreements',\n",
       "  'metadata': {'section': '1. Compensation',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Compensation'},\n",
       "  'chunk_size': 226},\n",
       " {'text': '‚Ä¢ Medical, dental, and vision insurance - 401(k) plan with company match up to 4% - Life and disability insurance - Professional development allowance ($5, 000 annually) - Security certification reimbursement program',\n",
       "  'metadata': {'section': '2. Benefits Package',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Benefits Package'},\n",
       "  'chunk_size': 216},\n",
       " {'text': '‚Ä¢ 0-2 years: 15 days annually - 3-5 years: 20 days annually - 6+ years: 25 days annually - Maximum accrual cap: 1.5x annual allowance',\n",
       "  'metadata': {'section': '1. Paid Time Off (PTO)',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 1. Paid Time Off (PTO)'},\n",
       "  'chunk_size': 133},\n",
       " {'text': '‚Ä¢ Bereavement leave: Up to 5 days - Jury duty: Paid as required by law - Military leave: In accordance with USERRA - Parental leave: 12 weeks paid at 100% salary',\n",
       "  'metadata': {'section': '2. Other Leave',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Other Leave'},\n",
       "  'chunk_size': 161},\n",
       " {'text': '‚Ä¢ Maintain highest ethical standards - Report conflicts of interest - Comply with all applicable laws and regulations - Protect company and customer confidential information',\n",
       "  'metadata': {'section': '6. Code of Conduct',\n",
       "   'subsection': '6.1 Professional Standards',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 6. Code of Conduct > Subsection: 6.1 Professional Standards'},\n",
       "  'chunk_size': 173},\n",
       " {'text': '‚Ä¢ Report safety concerns immediately - Follow all security protocols - Zero tolerance for workplace violence - Maintain required certifications',\n",
       "  'metadata': {'section': '2. Workplace Safety',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 2. Workplace Safety'},\n",
       "  'chunk_size': 143},\n",
       " {'text': 'Date: January 1, 2023',\n",
       "  'metadata': {'section': '8. Execution',\n",
       "   'subsection': '8.2 Employee Name: _ Employee Signature: Date: _ Authorized by: 8.3 /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.',\n",
       "   'document_title': 'Sample Legal Document',\n",
       "   'is_split_chunk': False,\n",
       "   'section_hierarchy': 'Section: 8. Execution > Subsection: 8.2 Employee Name: _ Employee Signature: Date: _ Authorized by: 8.3 /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.'},\n",
       "  'chunk_size': 21}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = process_legal_document(cleaned_text, document_title=\"Sample Legal Document\", debug=True)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c9345d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or time period - Exempt: Salaried employees exempt from overtime requirements - Non-exempt: Employees eligible for overtime compensation',\n",
       " 'metadata': {'section': '2. Employment Classification',\n",
       "  'document_title': 'Sample Legal Document',\n",
       "  'is_split_chunk': False,\n",
       "  'section_hierarchy': 'Section: 2. Employment Classification'},\n",
       " 'chunk_size': 332}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90111a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'document_title': 'Sample Legal Document', 'is_split_chunk': False}, page_content='DeepShield Systems Employee Handbook 2023'),\n",
       " Document(metadata={'section': '2. Employment Classification', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Employment Classification'}, page_content='‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temporary: Employees hired for a specific project or time period - Exempt: Salaried employees exempt from overtime requirements - Non-exempt: Employees eligible for overtime compensation'),\n",
       " Document(metadata={'section': '3. Workplace Policies', 'subsection': '3.1 Confidentiality and Intellectual Property', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Workplace Policies > Subsection: 3.1 Confidentiality and Intellectual Property'}, page_content=\"‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is not limited to: source code, security architectures, customer data, threat detection algorithms, and maritime protection protocols - Employees must report any potential security breaches or unauthorized access immediately\"),\n",
       " Document(metadata={'section': '2. Cybersecurity Compliance', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Cybersecurity Compliance'}, page_content='‚Ä¢ Mandatory participation in quarterly cybersecurity training - Strict adherence to the Company\\'s \"Clean Screen\" policy - Two-factor authentication required for all system access - Prohibition on use of unauthorized software or devices'),\n",
       " Document(metadata={'section': '3. Remote Work Policy', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Remote Work Policy'}, page_content=\"‚Ä¢ Eligible employees may work remotely subject to manager approval - Remote work must comply with DeepShield's security protocols - VPN and encrypted connections required for all remote access - Regular office presence required for collaborative projects\"),\n",
       " Document(metadata={'section': '1. Compensation', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Compensation'}, page_content='‚Ä¢ Pay periods: Semi-monthly on the 15th and last day of each month - Performance reviews conducted annually - Merit increases based on performance and market conditions - Equity compensation available per individual agreements'),\n",
       " Document(metadata={'section': '2. Benefits Package', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Benefits Package'}, page_content='‚Ä¢ Medical, dental, and vision insurance - 401(k) plan with company match up to 4% - Life and disability insurance - Professional development allowance ($5, 000 annually) - Security certification reimbursement program'),\n",
       " Document(metadata={'section': '1. Paid Time Off (PTO)', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 1. Paid Time Off (PTO)'}, page_content='‚Ä¢ 0-2 years: 15 days annually - 3-5 years: 20 days annually - 6+ years: 25 days annually - Maximum accrual cap: 1.5x annual allowance'),\n",
       " Document(metadata={'section': '2. Other Leave', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Other Leave'}, page_content='‚Ä¢ Bereavement leave: Up to 5 days - Jury duty: Paid as required by law - Military leave: In accordance with USERRA - Parental leave: 12 weeks paid at 100% salary'),\n",
       " Document(metadata={'section': '6. Code of Conduct', 'subsection': '6.1 Professional Standards', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 6. Code of Conduct > Subsection: 6.1 Professional Standards'}, page_content='‚Ä¢ Maintain highest ethical standards - Report conflicts of interest - Comply with all applicable laws and regulations - Protect company and customer confidential information'),\n",
       " Document(metadata={'section': '2. Workplace Safety', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Workplace Safety'}, page_content='‚Ä¢ Report safety concerns immediately - Follow all security protocols - Zero tolerance for workplace violence - Maintain required certifications'),\n",
       " Document(metadata={'section': '8. Execution', 'subsection': '8.2 Employee Name: _ Employee Signature: Date: _ Authorized by: 8.3 /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.', 'document_title': 'Sample Legal Document', 'is_split_chunk': False, 'section_hierarchy': 'Section: 8. Execution > Subsection: 8.2 Employee Name: _ Employee Signature: Date: _ Authorized by: 8.3 /s/ Dr. Marcus Chen Chief Executive Officer DeepShield Systems, Inc.'}, page_content='Date: January 1, 2023')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integration with LangChain vector stores\n",
    "def create_langchain_documents(chunks: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Convert chunks to LangChain Document objects for vector store ingestion.\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        doc = Document(\n",
    "            page_content=chunk[\"text\"],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "langchian_docs = create_langchain_documents(chunks)\n",
    "langchian_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eff903",
   "metadata": {},
   "source": [
    "Same issue in LangChain chunking that treats a newline as a seperate chunk. Seems to be an issue with the scanning/cleaning parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0efb668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 12\n",
      "\n",
      "First 3 chunks:\n",
      "============================================================\n",
      "\n",
      "Chunk 1:\n",
      "Metadata: {'document_title': 'Employee Handbook', 'is_split_chunk': False}\n",
      "Text (41 chars): DeepShield Systems Employee Handbook 2023...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Metadata: {'section': '2. Employment Classification', 'document_title': 'Employee Handbook', 'is_split_chunk': False, 'section_hierarchy': 'Section: 2. Employment Classification'}\n",
      "Text (332 chars): ‚Ä¢ Full-time Regular: Employees scheduled to work 40 hours per week - Part-time Regular: Employees scheduled to work less than 40 hours per week - Temp...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Metadata: {'section': '3. Workplace Policies', 'subsection': '3.1 Confidentiality and Intellectual Property', 'document_title': 'Employee Handbook', 'is_split_chunk': False, 'section_hierarchy': 'Section: 3. Workplace Policies > Subsection: 3.1 Confidentiality and Intellectual Property'}\n",
      "Text (371 chars): ‚Ä¢ All employees must sign and comply with the Company's Confidentiality and Intellectual Property Agreement - Proprietary information includes but is ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test chunking with the improved cleaned text\n",
    "chunker = LegalDocumentChunker(\n",
    "    max_chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    preserve_section_hierarchy=True\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_legal_document(cleaned_text, document_title=\"Employee Handbook\")\n",
    "chunks_with_context = chunker.create_section_hierarchy_context(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks_with_context)}\")\n",
    "print(f\"\\nFirst 3 chunks:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_context[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")\n",
    "    print(f\"Text ({chunk['chunk_size']} chars): {chunk['text'][:150]}...\")\n",
    "    print(\"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
